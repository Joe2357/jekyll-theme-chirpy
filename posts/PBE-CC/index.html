<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement" /><meta name="author" content="Joe2357" /><meta property="og:locale" content="en_US" /><meta name="description" content="2021 / 1 / 20 IMES 세미나" /><meta property="og:description" content="2021 / 1 / 20 IMES 세미나" /><link rel="canonical" href="https://joe2357.github.io/posts/PBE-CC/" /><meta property="og:url" content="https://joe2357.github.io/posts/PBE-CC/" /><meta property="og:site_name" content="Joe2357" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-11-26T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Joe2357" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Joe2357"},"headline":"PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement","dateModified":"2021-02-15T16:55:23+09:00","datePublished":"2020-11-26T00:00:00+09:00","description":"2021 / 1 / 20 IMES 세미나","url":"https://joe2357.github.io/posts/PBE-CC/","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://joe2357.github.io/posts/PBE-CC/"},"@context":"https://schema.org"}</script><title>PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement | Joe2357</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/profile.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Joe2357</a></div><div class="site-subtitle font-italic">공부하는 블로그</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Joe2357" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['deviljoe996','gachon.ac.kr'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Nov 26, 2020, 12:00 AM +0900" > Nov 26, 2020 <i class="unloaded">2020-11-26T00:00:00+09:00</i> </span> by <span class="author"> Joe2357 </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Feb 15, 2021, 4:55 PM +0900" > Feb 15 <i class="unloaded">2021-02-15T16:55:23+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="10564 words">58 min</span></div></div><div class="post-content"><blockquote><p>2021 / 1 / 20 IMES 세미나</p></blockquote><h2 id="abstract">Abstract</h2><ul><li>cellular network는 복잡해지고 과밀되어짐<ul><li>delay, jitter 등의 문제 발생</ul><li><i>PBE-CC</i> : sender가 정확하고 급격하게 반응할 수 있도록 하는 최신 5G radio 혁신 기반 congestion control 알고리즘<ul><li>5G radio 혁신 : wireless capacity를 급격히 늘리거나 줄일 수 있는 bandwidth<li>구현 언어 : C</ul></ul><h2 id="introduction">Introduction</h2><ul><li>downlink end-to-end data flow : cellular last hop에서 종료<ul><li>대부분의 delay, delay variation, packet loss, bandwidth 제한 등 발생</ul><li>end-to-end 혼잡도를 측정하기 위해서는 endpoint가 제일 좋음<ul><li>mobile endpoint가 무선 last hop의 혼잡도를 측정<li>세분화된 측정값을 API를 통해 전송계층과 app에게 공급해야함</ul><li>congestion control 알고리즘이 wireless 상황에서 겪는 <u>3가지 문제점</u><ul><li>wireless = 공유 매체<ul><li>같은 cell tower에 있는 다른 user들이 capacity의 변화를 느낄 수 있음<li>오늘날 ACK 기반 프로토콜에서는, 재전송 승인 흐름에 반영되는데 <strong>시간이 걸림</strong></ul><li>높은 throughput, 낮은 queueing delay를 위해 <strong>누구도 관측하기 힘든</strong> wireless cell link capacity의 급격한 변화에 신속대응해야함<ul><li>capacity 변화 원인 : <strong>carrier aggregation</strong><ul><li>cellular network가 2개 이상의 base station의 bandwidth를 집계하여 총 bandwidth로 사용하는 기술<li>base station을 추가하거나 제거 -&gt; <u>user가 사용 가능한 wireless capacity가 갑자기 변경</u></ul></ul><li>wireless channel 품질 = dynamic<ul><li>원인 : user mobility, multipath propagation, 인접 cell tower interference 등<li>특정 user의 cellular link가 지원하는 wireless data rate를 wireless channel <i>coherence time</i>으로 알려진 시간 척도에 걸쳐 변경<ul><li>차량 속도 정도면 ms 단위</ul><li>base station간 handover : state를 migrate해줘야함<ul><li>설계에는 반영되지 않았음</ul></ul><li>앞서 말한 요인들에 의해 악화되어감</ul><li>carrier aggregation : end-to-end connection은 aggregated cell의 dynamics에 의해 변동 발생<ul><li>통계적 multiplexing에 의한, capacity를 평활화할 수 있는 것보다 적음</ul><li>base station, mobile endpoint 모두 변동을 관측할 수 있음<ul><li>현재 cellular physical-layer 설계 : <strong>자신의 channel에 할당된 message만 해석</strong><ul><li>다른 user의 channel capacity를 추적하여 capacity를 식별할 수 없음</ul></ul><li>PBE-CC : Congestion Control based of Physical-layer Bandwidth measurements taken at the mobile Endpoint<ul><li>2개의 module로 이루어져 있는 <strong>cross-layer 설계</strong><ul><li>TCP BBR 기반 <strong>end-to-end congestion control 알고리즘</strong><ul><li>가능한 경우 정확한 congestion control을 위해 sender를 수정<li>TCP BBR : 구글에서 만든 congestion control 알고리즘<ul><li>CUBIC의 대용<li>bottleneck link의 buffer를 가득 채우지 않으며 동작<li>packet loss을 줄이고 transport delay를 최소화<li>CUBIC : 손실 기반 방식 알고리즘 ( packet loss를 관측하고 송신 억제 )</ul></ul><li>mobile device를 위한 wireless physical-layer capacity 측정 module<ul><li>end-to-end congestion control 활용</ul></ul><li><strong>주요 혁신</strong> : wireless cellular link의 매우 정확한 capacity 측정<ul><li>link : 변화를 ms 단위로 세분화하여 추적<li>send rate를 available wireless capacity ( bottleneck ) 와 일치시켜 sender의 send rate를 더 정밀하게 제어할 수 있음<li>wireless capacity의 변화에 대해 PBE-CC가 신속하게 대응<ul><li>capacity가 증가한 경우<ul><li>새로 생긴 idle capacity를 감지<li>sender가 사용할 수 있는 rate를 증가시킬 수 있음</ul><li>capacity가 감소한 경우<ul><li>send rate를 억제<li><strong>queueing delay를 피할 수 있음</strong> // drill-down 실험</ul></ul></ul></ul><li>wireless cellular link가 대체로 end-to-end link의 bottleneck link<ul><li>PBE-CC : wireless-aware 정밀 congestion control 활용<ul><li>sender의 pace를 정확하게 제어 + wireless link를 공유하는 user 수를 고려<li>-&gt; 동일한 초기 가정을 수행<li>=&gt; <strong>각 PBE-CC sender가 user간에 wireless capacity를 전반적으로 공정하게 분배하는 load 제공</strong><li>추가적인 개선을 통해, sender : connection 시작 시 이 target에 부드럽게 접근<ul><li>다른 user들이 이에 대응하고 조정할 시간을 줌</ul><li>예측하지 못한 one-way packet delay 증가를 감지하면, sender가 받은 ACK packet의 속도를 기반으로 한, bottleneck을 조사하는 BBR과 같은 매커니즘을 대신 사용</ul></ul><li>C++로 구현<ul><li>mobile wireless frontend : physical-layer capacity 측정을 위해 주파수 대역을 decode해야함<ul><li>펌웨어가 제공하지 않음</ul><li>USRP를 이용하여 누락된 펌웨어 기능을 구현</ul><li>Pantheon이라는 training guide를 이용하여 evaluation<ul><li>선두적인 알고리즘 : BBR, CUBIC<li>최근에 고안된 알고리즘 : Copa, PCC, PCC-Vivace</ul><li>실험 평가 : delay와 throughput 측정<ul><li>고정된 user device를 이용<li>실험 조건<ul><li>실내 / 실외<li>busy / quiet</ul><li>추가 실험 조건<ul><li>같은 user device 조건 // 고정된 user device<li>wireless network capacity에 대한 경쟁<ul><li>통제된 경쟁자<li>통제되지 않은 background traffic 경쟁자</ul><li>100 ms 단위로 측정된 <u>모든 throughput과 delay order statistics</u>를 개별적으로 기록</ul></ul><li>결과 ( 15개의 idle cellular link, 25개의 busy link )<ul><li>BBR : 더 좋은 throughput / delay 낮춤<li>Verus : 보다 상당한 이득<li>Copa : delay에서는 약간의 손해 / throughput 11배</ul></ul><h2 id="related-work">Related Work</h2><h5 id="end-to-end-congestion-control">End-to-End congestion control</h5><ul><li>loss 기반 알고리즘 : 높은 throughput / 과도한 delay<li>delay 기반 알고리즘 : ACK delay, ACK compression, network jitter 발생<ul><li>network capacity 활용도 감소</ul><li>위의 방법들은 모두 concurrent loss 기반 알고리즘보다 <u>낮은 capacity 활용률</u><li>다른 제안 : 학습된 알고리즘을 사용<ul><li>특정 목적 함수 최적화<li>인간이 만든 규칙보다 더 나은 congestion control 작업 생성</ul><li>온라인 학습 : network 활용도가 현저히 떨어지는 해답으로 수렴<li>BBR : Kleinrock의 최적 운영 지점에 대한 수렴이 목표<ul><li>bottleneck bandwidth 및 $RT_{prop}$의 추정치를 기반으로 throughput 최대화, delay 최소화<li>테스트한 알고리즘들 중 최고의 성능이었으나, <strong>network를 충분히 활용하지 못함</strong><ul><li>과도한 지연 초래 ( capacity 추정치가 조잡함 )</ul></ul></ul><h5 id="end-to-end-congestion-control-for-cellular-networks">End-to-End congestion control for cellular networks</h5><ul><li>cellular link를 블랙박스로 취급하고, link capacity를 추론하기 위해 throughput, packet delay, loss 통계를 사용<ul><li>Raven : multipath TCP를 사용<ul><li>multipath를 통한 중복 data 전송<li>interactive video latency 단축</ul><li>PROTEUS : 현재 throughput, loss, one-way delay 수집<ul><li>회귀 트리 사용<li>미래 network 성능 예측 목적</ul><li>PropRate : BBR의 주기적인 bandwidth probing을 연속 probing으로 대체<ul><li>packet size, packet send/receive time을 사용하여, 예상 receive rate를 중심으로 send rate를 진동시킴 ( 왔다갔다 )</ul><li>Sprout : packet 도착 시간을 활용<ul><li>network 경로의 불확실한 dynamics 추론<li>link capacity 예측</ul><li>ExLL : send rate 조절<ul><li>packet 도착 패턴 + cellular bandwidth 사용 간의 간계 modeling</ul><li>Verus : cellular network dynamics 추론 대신, target send window size와 인지된 endpoint간 delay 사이의 관계를 포착하는 <u>delay profile</u> 학습 목적<li>end-to-end 통계에 의존할 때, 위의 알고리즘은 <strong>capacity 추정 부정확성</strong>에 시달림<ul><li>network dynamics에 민감</ul></ul><li>PBE-CC : wireless channel을 <strong>직접 측정</strong><ul><li>보다 세밀한 capacity 추정<li>탁월한 성능 제공</ul></ul><h5 id="cellular-aware-congestion-control-proposals">Cellular-aware congestion control proposals</h5><ul><li>ABC, IETF MTG 표준 초안 : sender에게 best rate를 명확히 전달하기 위해 <u>mobile base station의 수정을 제안</u><ul><li>고성능에 중요한 capacity monitor 설계에 대한 설명은 없음</ul><li>CQIC : 3G capacity 추정치를 추출하여 cross-layer 설계를 시작<ul><li>여전히 세분성이 부족</ul><li>piStream, CLAW : 신호 강도 측정에 사용된 resource block을 예측하는 model 만듬<ul><li>CLAW : 웹 검색 작업 속도 높임<li>piStream : 비디오 작업 속도 높임<li>자체 측정에 따르면 신호 강도 예측력이 상당히 제한되어있음</ul><li>PBE-CC : control 채널 metadata를 직접 decode하여 추정치가 아닌 <strong>정확한 bandwidth 활용도 제공</strong></ul><h5 id="cellular-phy-layer-monitoring-tools">Cellular PHY-layer monitoring tools</h5><ul><li>QXDM, MobileInsight : 단일 mobile user를 위한 control 메세지를 추출<ul><li>cell tower capacity 점유에 대한 순 정보를 제공할 수는 없음<li><strong>PBE-CC는 제공할 수 있다</strong></ul><li>BurstTracker : end-to-end 연결의 bottleneck 지점을 찾음<li>LTEye, OWL : control message decode<ul><li>PBE-CC와는 달리 carrier aggregation 및 고급 MIMO 표준에서 작동하지 않음</ul><li>위의 도구들은 congestion control 알고리즘 설계가 부족함</ul><h2 id="lte5g-new-radio-primer">LTE/5G New radio primer</h2><ul><li>Frequency division duplexing ( FDD ) : 주파수 분할 이중화<ul><li>cellular 사업자가 가장 많이 사용하는 mode</ul><li>LTE : OFDMA 채택<ul><li>frequency bandwidth를 180KHz로, 시간을 0.5ms slot으로 나눔<li>가장 작은 1칸 = physical resource block (PRB)<ul><li>user에게 할당할 수 있는 가장 작은 block</ul><li>2개의 slot을 group화하여 1ms subframe 제작<ul><li>subframe 내에서 두 slot의 PRB 할당은 동일</ul><li>transport block (TB) : 하나의 subframe을 통해 전송되는 data<ul><li>할당된 PRB만큼 크기, user의 wireless physical data rate에 의해 달라짐</ul><li>base station : wireless control channel에서 전송되는 control message를 통해 mobile user에게 알림<ul><li>bandwidth 할당 ( 할당된 PRB 양과 위치 )<li>wireless bit rate<ul><li>변조 및 코딩 방식 (MCS)<li>공간 stream 수</ul></ul><li>mobile user : TB를 decode하기 전에 subframe의 control message를 먼저 decode</ul></ul><h5 id="carrier-aggregation">Carrier aggregation</h5><ul><li>base station : component carrier 또는 primary cell을 통해 mobile user에게 data 전송<ul><li>전달해야하는 data의 양이 많으면, capacity를 추가로 확보하기 위해 secondary cell 활성화</ul><li>cellular network : 각 user에 대한 aggregated cell들의 목록을 유지관리<ul><li>필요한 경우 순차적으로 활성화<li>불필요해지는 경우 비활성화</ul><li>예시<ul><li>receiver인 mobile user에게 primary cell의 최대 capacity을 초과하는 data 전송<ul><li>40Mbits 고정 제공 load로 2초동안<li><strong>모든 bandwidth가 이곳에 할당되더라도 packet buffering 발생</strong></ul><li>cellular network : 높은 data rate의 user를 감지<ul><li>data 전송을 도울 secondary cell 활성화<li>secondary cell이 활성화되면 합계 capacity가 40Mbit보다 커짐<ul><li><strong>쌓인 queue가 점차 비워질 것</strong></ul></ul><li>sender가 send rate를 primary cell의 capacity보다 낮춤 (6Mbit/s)<ul><li>secondary cell 비활성화</ul></ul></ul><h5 id="cellular-retransmission-and-reordering">Cellular retransmission and reordering</h5><ul><li>잘못된 TB 전송 : 8ms 이후에 재전송 ( 8개의 subframe 이후 )<ul><li>block 순서를 맞추기 위해, 없어진 순서 이후의 도착한 block들은 <strong>buffering</strong><li>재전송 성공 =&gt; 버퍼링된 모든 block과 재전송된 block을 상위계층에 올림<ul><li>TB에서 packet을 추출</ul></ul><li>재전송 : 오류가 있는 TB 내의 전송계층 packet에 8ms delay 도입<ul><li>receiver 측의 buffering, reordering 작업 : 8ms보다는 감소하는 delay 도입 (7ms ~ 0ms)</ul><li>재전송마저 실패하는 경우 : 최대 3번까지 재전송을 반복<ul><li>지연 패널티 : 8ms * 반복 횟수 (최대 3번)</ul></ul><h2 id="design">Design</h2><ul><li>PBE-CC : cellular network를 통과하여 mobile endpoint에서 종료되는 flow를 위한 <strong>rate 기반 end-to-end congestion control 알고리즘</strong><ul><li>mobile user : base station의 available wireless capacity에 대한 자세한 정보를 포함하는 cellular physical control channel을 decode<li>그 양을 ms 단위로 정확히 추정 가능</ul><li>sender : mobile user가 명시적으로 feedback하는 <u>bottleneck capacity 추정값</u> 또는 <u>receiver로부터의 ACK 존재 여부</u>를 기준으로 send rate 제어<li>PBE-CC : bottleneck 위치가 wireless hop인 경우 사용 가능<ul><li>congestion을 일으키지 않고 available capacity를 확보하여 send rate 높일 수 있음<li>다른 mobile user가 capacity를 줄이면 send rate 또한 줄임</ul><li>end-to-end connection : 2가지 경우의 network state에 직면 ( internet bottleneck link와 cellular link의 상대적 capacity에 따라서 )<ul><li>대부분의 경우 : wireless cellular link가 bottleneck인 경우 = <strong>wireless bottleneck 현상</strong><ul><li>PBE-CC mobile user : cellular physical control channel을 decode -&gt; 전체 connection의 bottleneck을 ms 단위로 추정하고 추적 가능<li>sender : user가 feedback하는 bottleneck capacity와 send rate를 일치시킴 -&gt; <strong>정확하게 capacity 활용</strong>, buffering 최소화</ul><li>이외의 경우 : 인터넷 bottleneck이 wireless cellular link capacity보다 작은 경우 = <strong>internet bottleneck 현상</strong><ul><li>PBE-CC : cellular 맞춤형 BBR같은 congestion control 알고리즘 사용<ul><li>bottleneck의 capacity를 공정하게 share하기 위해, internet bottleneck을 다른 flow들과 공정하게 경쟁</ul></ul><li>PBE-CC : 2가지 상태에서 발생할 수 있는 변화를 추적<ul><li>그에 따라 sender의 action을 제어</ul></ul><li>Kleinrock : bandwidth 최대화, delay 최소화하는 <strong>operating point</strong>가 개별 connection과 network 전체에 <u>최적임을 입증</u><ul><li>operating point : pipe를 가득 채워야 한다는 것이 특징<li>PBE-CC의 목적 == BBR의 목적 // <u>pipe 채우기, network 내부 buffering 최소화</u></ul><li>PBE-CC : BDP로 돌아다니는 data의 양을 limit<ul><li>BDP : bandwidth delay product ( 대역폭 지연 제품 ) - 아래의 것들에 의해 계산됨<ul><li>$RT_{prop}$ : 추정 round-trip propagation<li>congestion window bottleneck capacity</ul><li>sender : user의 feedback이 지연되더라도 과도한 양의 packet을 전송하지 않음 -&gt; <strong>네트워크의 queueing을 최소화</strong> // low latency</ul></ul><h3 id="connection-start-linear-rate-increase">Connection start: Linear rate increase</h3><ul><li><p>connection 시작 - sender : linear rate increase 실행 ( bottleneck에 공정하게 접근하기 위함 )</p><ul><li><p>control channel decode : cell bandwidth를 공유하는 다른 user들의 수를 알 수 있음</p><li><p>공정하게 나누는 것이 기본 idea이므로, 1명이 사용 가능한 capacity = $P_{exp}$</p>\[P_{exp} = P_{cell}/N\] \[\begin{aligned} &amp;P_{exp} =\text{예상 fair-share bandwidth}\\ &amp;P_{cell} =\text{cell의 총 PRB}\\ &amp;N =\text{user 수} \end{aligned}\]<li><p>예상 fair-shared send rate = $C_f$</p>\[C_f=R_w\times P_{exp}\] \[\begin{aligned} &amp;C_f=\text{예상 fair-share send rate}\\ &amp;R_w=\text{wireless physical data rate}\cdots\text{(PRB당 비트단위 포함)}\\ \end{aligned}\]</ul><li><p>3 RTT까지 send rate를 $C_f$까지 linear하게 증가시킴</p><ul><li>mobile user : ms단위마다 $C_f$를 update, ACK마다 server에게 rate를 전송</ul><li><p>이점</p><ul><li>bursty traffic 방지<li>cell tower, tower를 공유하는 다른 user들에게 증가하는 traffic에 반응할 시간을 남겨둠<ul><li>cell tower : mobile user의 send rate가 증가하면, <strong>그에 비례하여 bandwidth를 추가 할당</strong><ul><li>이외의 user들은 bandwidth가 감소</ul><li>user : bandwidth가 감소하면 즉시 감지하고 <strong>그들의 sender에게 rate를 낮출 것을 요구하는 신호 보냄</strong></ul><li>모든 PBE-CC user는 동등하게 공유되는 bandwidth와 균형을 이루는 경향</ul><li><p>fair-share approaching state : 2개 이상의 component carrier가 활성화되면, <strong>각 aggregated cell에 대해 개별적으로 target send rate를 계산</strong></p><ul><li>이후 그것을 $C_f$로 합산</ul><li><p>congestion avoidance 단계에서 더 많은 carrier가 활성화된 경우 : PBE-CC는 <u>fair-share approaching process 재실행</u></p><li><p>user의 fair-shared send rate가 $C_f$에 도달하면, linear rate increase를 그만두고, congestion avoidance로 진입</p><li><p>bottleneck이 internet 내부에 있는 경우 : $C_f$만큼의 rate를 달성할 수 없음</p><ul><li>cell tower에서 달성한 throughput은 $C_f$보다 적음<li>sender의 offered load가 증가 -&gt; <strong>end-to-end packet delay 증가</strong></ul><li><p>mobile user: 아래의 상황을 감지하여 <strong>linear rate increase 종료, cellular 맞춤형 BBR로 전환</strong> // internet의 congestion을 처리하기 위함</p><ul><li>$RT_{prop}$에 대해 receive rate가 증가하지 않는 것<li>one-way packet delay가 offered load가 증가함에 따라 단조롭게 증가하는 것</ul></ul><h3 id="steady-state-congestion-avoidance">Steady state: Congestion Avoidance</h3><ul><li>wireless link가 bottleneck인 경우 : sender가 추정 wireless capacity까지 rate 증가시킴 (A)<li>connection startup과 유사하게, wireless bottleneck에서 internet bottleneck으로의 전환 가능성을 식별 (B)<ul><li>만약 일어난다면, cellular 맞춤형 BBR (C)로 전환하여 bottleneck에서의 flow와 공정한 경쟁을 벌임</ul></ul><h4 id="a-wireless-bottleneck-state">A: Wireless Bottleneck state</h4><ul><li><p>PBE-CC mobile user : available cellular wireless capacity인 $C_p$를 추정</p>\[C_p=\sum\limits_{i=1}^{N_{cell}}(R_{w,i}\times (P_{a,i}+\frac 1 N_i P_{idle,i}))\] \[\begin{aligned} &amp;C_p=\text{available cellular wireless capacity}\\ &amp;N_{cell}=\text{user에게 활성화된 cell의 개수}\\ &amp;R_{w,i}=\text{user에게 i번째 cell이 제공하는 wireless data rate}\\ &amp;P_{a,i}=\text{user에게 i번째 cell에서 할당된 PRB의 개수}\\ &amp;N_i=\text{i번째 cell에 있는 user의 수}\\ &amp;P_{idle,i}=\text{i번째 cell에 있는 idle PRB의 개수} \end{aligned}\] \[P_{idle,i}=P_{cell,i}-\sum\limits_{j=1}^{Ni}P^j_{a,i}\] \[\begin{aligned} &amp;P_{idle,i}=\text{i번째 cell에 있는 idle PRB의 개수}\\ &amp;P_{cell,i}=\text{i번째 cell에 있는 PRB의 개수}\\ &amp;N_i=\text{i번째 cell에 있는 user의 수}\\ &amp;P^j_{a,i}=\text{i번째 cell의 user j에게 할당된 PRB의 개수} \end{aligned}\]<li><p>추정치를 원활하게 하기 위하여, 계산한 $R_{w,i}, P_{idle,i}, P_{a,i}$를 평균화</p><ul><li>가장 최근의 $RT_{prop}$ subframe을 이용하여 계산한 값들의 평균<li>만약 connection RTT가 40ms라면 40개의 subframe을 이용</ul><li><p>추정 wireless capacity $C_p$를 이해하기 위해 $C_p$ 식의 구성요소를 고려해야함</p><ul><li>$R_w$ = mobile user가 다양한 channel 품질에 의한 capacity 변화를 추적할 수 있도록 함<li>mobile user : 자신에게 할당된 PRB의 수 $P_a$를 추적하여 새로운 user의 출현에 반응<ul><li>새로운 user가 traffic을 수신하면 기존 user는 $P_a$를 줄임<li>할당된 PRB 수가 적어짐을 감지하면, sender는 추정된 감소 capacity에 맞춰 send rate를 줄임</ul></ul><li><p>idle PRB인 $P_{idle}$이 무선 bottleneck connection에 쓰이고 있는 cell에 나타난다면, <strong>모든 PBE-CC user는 control message를 decode하여 이를 감지</strong></p><ul><li><p>sender에게 idle PRB를 fair-share한 부분을 갖기 위해 <u>send rate를 올릴 것을 알림</u> ( $P_{idle} / N$ 만큼 )</p><li><p>idle PRB가 등장하는 사례</p><ol><li><p>한 user가 flow를 끝낸 경우</p><li><p>여러가지 요인에 의해 user flow의 data rate가 감소한 경우</p><ol><li>internet의 congestion<li>app 자체의 congestion<li>cellular network에 의한, 한 cell에서 다른 aggregated cell로의 traffic 이동</ol></ol><li><p>idle PRB가 남았다고 해서 <strong>항상 전부를 나눠갖는 것이 아님</strong></p><ul><li>1ms마다 자신에게 추가 할당할 수 있는 $P_{idle} / N$ 만큼의 fair-share를 가져감<li>app 자체 congestion에 의해 data rate가 줄어든 user는 fair-share를 가지지 않음 -&gt; <strong>그 칸은 계속 idle인 상태 유지</strong><ul><li>계속 idle인 PRB는 계속해서 detect되고, fair-share됨</ul></ul><li><p><strong>network는 flow가 없는 user를 제외하고 모든 user가 idle bandwidth를 모두 잡은 상태로 수렴</strong></p></ul></ul><h5 id="cross-layer-bit-rate-translation">Cross-layer bit rate translation</h5><ul><li><p>$C_f, C_p$ : MAC계층 재전송 + protocol header overhead에 의한 <u>전송계층 data rate와는 다른</u> <strong>wireless physical-layer capacity</strong></p><ul><li>PBE-CC : 예상 physical-layer capacity $C_p$를 전송계층 goodput인 $C_t$로 변환, server에게 send rate를 설정할 것을 feedback</ul><li><p>cell : <i>new-data-indicator</i> 사용 -&gt; 재전송된 TB를 나타냄</p><ul><li>재전송 overhead, protocol overhead를 별도로 측정 가능</ul><li><p>TB 오류 확률이 재전송 overhead를 결정</p>\[\text{TB error rate}=1-(1-p)^L\] \[\begin{aligned} &amp;p=\text{TB 안의 각 bit에 대한 bit error rate}\cdots\text{(확률은 iid라고 가정)}\\ &amp;L=\text{TB의 크기} \end{aligned}\]<li><p>PBE-CC : $C_p$와 $C_t$사이의 관계를 정리</p>\[C_p=C_t+C_t\times(1-(1-p)^L)+γ\times C_p\] \[\begin{aligned} &amp;C_p=\text{추정 physical-layer capacity}\\ &amp;C_t=\text{전송계층 goodput}\\ &amp;p=\text{TB 안의 각 bit에 대한 bit error rate}\\ &amp;L=\text{TB의 크기 (1개의 subframe 내의 bit 수)}\\ &amp;γ=\text{protocol overhead (6.8%)} \end{aligned}\]<li><p>SINR을 이용하여 $p$를 추정</p><ul><li><p>SINR (<i>signal to interference noise ratio</i>)</p>\[SINR(x)=\frac P {I+N}\] \[\begin{aligned} &amp;P=\text{interest 신호 세기}\\ &amp;I=\text{간섭 신호 세기}\\ &amp;N=\text{랜덤 noise term} \end{aligned}\]<li><p>이후 $C_p$를 제공하여 방정식을 풀고 $C_t$를 추정</p><li><p>계산을 빠르게 하기 위해, look-up table을 만들어 변화를 저장</p></ul><li><p>한 PBE-CC user가 fair-share capacity를 가져가면..</p>\[L=C_t\times 10^{-3}\] \[\begin{aligned} &amp;L=\text{TB의 크기 (1 subframe 내의 bit수 ( bit/} 10^{-3} \text{ms ))} \\ &amp;C_t=\text{전송계층 goodput} \end{aligned}\]</ul><h5 id="handling-control-traffic">Handling control traffic</h5><ul><li>PBE-CC : 모든 active user에게 fair-share 무선 bandwidth 제공 목적<ul><li>실험 결과<ul><li>상당수의 user들이 data를 위해 활성화되어있는 것이 아님<li>base station, mobile에 의해 공유된 network parameter를 update하는 것을 목적으로 활성화되어있었음<ul><li>다양한 시간대의 period<li>aggregated cell의 list<li>많은 pricing, 보안 관련 parameter</ul><li>이러한 user가 있어서, 각 시점에서 감지된 active user는 커질 수 있음<ul><li>측정치 : 40ms 간격동안 평균 15.8명, 최대 28명의 active user</ul><li>위의 user들은 fair-share capacity 계산에서 제외<ul><li>이 user들을 위한 소량의 bandwidth를 cell tower에서 할당<li>여기서 할당한 bandwidth의 감소 $P_a$를 추적<li>추적된 값만큼 send rate 줄임</ul></ul></ul><li>control traffic : 적은 수의 PRB를 차지하고, 적은 시간 동안만 활성화됨<ul><li>68.2% user가 4개의 PRB 점유, 1개의 subframe에서 활성화<li>그중에서 95%가 base station으로부터 control traffic 수신중</ul><li>PBE-CC monitor : 활성화 시간 duration (subframe), 할당된 bandwidth (PRB)의 임계값을 기반으로 update에만 active된 user들을 필터링<ul><li>이후 감지된 user들의 수는 상당히 감소<ul><li>40ms 간격 내에서 15 -&gt; 1.3명으로 현저히 감소</ul><li>bandwidth를 위해 동시에 경쟁하는 user의 수는 최대 7명으로 관찰</ul><li>$N$ : 방정식 (2), (3) 에서 threshold를 적용한 후의 active user 수로 설정<ul><li>그러나 방정식 (4)에서 idle PRB를 계산하기 위해서는 모든 user로써 계산</ul></ul><h4 id="b-switching-between-bottleneck-states">B: Switching between Bottleneck states</h4><ul><li><p>sender offered load가 internet bottleneck capacity를 초과하는 경우 : packet queuing 발생</p><ul><li><p>PBE-CC : wireless bottleneck state -&gt; internet bottleneck state로 전환</p><li><p>즉각적인 one-way packet delay가 threshold를 초과하는 경우 switch 실행</p><ul><li><p>임계값 : server &lt;-&gt; client 간의 one-way propagation delay으로 설정해야함</p>\[D_{th}=D_{prop}\] \[\begin{aligned} &amp;D_{th}=\text{threshold delay}\\ &amp;D_{prop}=\text{one-way propagation delay} \end{aligned}\]<li><p>PBE-CC : $D_{prop}$을 10초 window에서의 최소 delay로 추정</p><ul><li>BBR의 round-trip propagation delay 추정 method를 불러옴<li>추정 패킷 delay가 10초동안 일정하게 유지된다면, BBR처럼 buffer를 뺌<ul><li>delay를 실제 $D_{prop}$값으로써 update</ul></ul></ul></ul><li><p>이론적인 threshold는 실제로는 제대로 작동하지 않음</p><ul><li>재정렬 작업에 의함<li>sender offered load가 높은 경우<ul><li>mobile user가 재정렬 buffer에 자주 buffering<li>packet delay 크게 변동</ul><li>offered load가 높아지면 TB error rate 또한 증가<li>mobile user : 재정렬 buffer에 더 많은 packet 저장<ul><li>수신된 packet 수의 증가, 8ms 재전송 delay 도입</ul><li>재전송 X, buffering X인 packet은 항상 있음<ul><li>최소 delay : one-way propagation delay를 캡처</ul></ul><li><p>분석에 의해 switching 임계점을 계산</p>\[D_{th}=D_{prop}+3\times8+3\] \[\begin{aligned} &amp;D_{th}=\text{switching threshold}\\ &amp;D_{prop}=\text{minimum delay}\\ &amp;3\times8=\text{TB는 최대 3번 재전송 가능}\\ &amp;3=\text{network jitter (실험 중에 94.1%의 확률로 &lt;= 3)} \end{aligned}\]<li><p>PBE-CC : delay 임계값을 초과하는 delay가 있는 연속 packet 수에 대한 임계값을 추가</p><ul><li><p>jitter 완화, 견고성 개선 목적</p><li><p>현재 data rate에 따라 6개의 subframe에 걸쳐 전송할 수 있는 패킷 수 $N_{pkt}$를 정의</p>\[N_{pkt}=6\times C_t/MSS\] \[\begin{aligned} &amp;N_{pkt}=\text{6개의 subframe으로 전달할 수 있는 packet의 수}\\ &amp;C_t=\text{현재 전송계층 capacity}\\ &amp;MSS=\text{최대 segment 크기} \end{aligned}\]</ul><li><p>PBE-CC : server와 mobile user간의 동기화가 필요하지 않음</p><ul><li>relative delay (현재 propagation delay와 threshold값의 차이) 를 기반으로 결정</ul></ul><h4 id="c-internet-bottleneck-state">C: Internet bottleneck state</h4><ul><li><p>PBE-CC : internet 내부의 bottleneck capacity와 일치하는 rate를 조사하기 위해 cellular 맞춤형 BBR로 전환</p><li><p>BBR sender : connection의 bottleneck bandwidth를 $BtlBw$로 설정</p><ul><li><p>$BtlBw$ : 전송 flow에 사용할 수 있는 BBR의 예상 bottleneck bandwidth</p><ul><li>최근 10개의 RTT의 최대 delivery rate로 추정</ul><li><p>sender : 자신의 offered rate를 아래와 같이 설정</p>\[\text{offered rate} = \text{pacing_gain} \times BtlBw\]<li><p><i>pacing_gain</i> : 3가지 값 중 하나로 설정됨</p><ul><li>1.25 ( available idle bandwidth를 probing하는 경우 )<li>0.75 ( 이전 probing 기간동안 buffering된 packet을 drain하는 경우 )<li>1 ( 그 이외의 경우 )</ul><li><p>ProbeBW 상태 : bandwidth를 조사하기 위해 8단계로 이루어진 주기를 반복</p><ul><li>각 단계의 길이 : $RT_{prop}$</ul><li><p>PBE-CC : 처음에 BBR의 ProbeBW 상태로 돌입</p><ul><li>ProbeBW, ProbeRTT, StartUp, Drain 상태를 번갈아가며, <strong>BBR의 ProbeBR과 동일한 control logic을 따름</strong></ul></ul></ul><h5 id="wireless-aware-bbr-like-probing">Wireless-aware, BBR-like probing</h5><ul><li><p>PBE-CC : internet bottleneck이 지원하는 더 높은 data rate 조사</p><ul><li>cellular wireless link의 fair-share send rate도 고려</ul><li><p>BBR의 bandwidth probing scheme 채택</p>\[C_{probe}=min(1.25BtlBw, C_f)\] \[\begin{aligned} &amp;C_{probe}=\text{probing rate}\\ &amp;1.25=\text{BBR의 probing pacing_gain}\\ &amp;BtlBw=\text{bottleneck bandwidth}\\ &amp;C_f=\text{wireless link 최대 fair-share capacity} \end{aligned}\]<li><p>mobile user : internet bottleneck이 감지되면 $C_f$를 sender로 재전송</p><li><p>PBE-CC : probing 단계 이후 drain phase 실행 -&gt; <u>모든 버퍼링 패킷 배출</u></p><ul><li>BBR와 유사함<li>internet bottleneck이 감지됨 -&gt; <strong>이미 packet queue가 형성되어있음</strong><ul><li>해당 상태로 전환 전에, 1 $RT_{prop}$시간 동안 추가 drain phase 실행<li>send rate를 $0.5BtlBw$로 설정<ul><li>남은 capacity $0.5BtlBw$는 drain하기 위해 남겨둠</ul></ul></ul></ul><h5 id="switching-back-to-wireless-bottleneck-state">Switching back to wireless bottleneck state</h5><ul><li>PBE-CC : network에 <u>어떠한 packet queuing도 존재하지 않은 상태로</u> <strong>send rate가</strong> $C_f$<strong>에 도달하면</strong> internet bottleneck 상태를 빠져나옴<ul><li>mobile user : 자신이 측정한 threshold $D_{th}$보다 적은 delay의 연속 packet $N_{pkt}$을 관측한 경우</ul><li>internet bottleneck state를 빠져나와 wireless bottleneck state로 전환<ul><li>network가 다시 internet bottleneck state가 될 때까지 상황 유지</ul></ul><h3 id="fairness-and-tcp-friendliness">Fairness and TCP-friendliness</h3><ul><li><p>PBE-CC : BBR 알고리즘을 보수적으로 수정</p><ul><li><u>internet bottleneck 현상을 공유하는 flow과 경쟁에서 BBR보다 덜 공격적</u></ul><li><p>wireless bottleneck 상태 : 각 PBE-CC user는 경쟁 user의 수를 알고 있음</p><ul><li>fair-share cellular wireless capacity로 빠르게 수렴</ul><li>cellular physical control channel message를 decode<ul><li>각 aggregated cell에서 capacity 사용량을 확인하여 fair-share capacity를 계산<li>계산한 capacity와 sender의 send rate를 맞추도록 sender를 가이드</ul><li>기존의 end-to-end congestion control 알고리즘 : 더 복잡한 probing과 backoff step을 거쳐 fair-share를 probe해야함<ul><li><strong>효율성이 떨어진다</strong></ul><li>cell tower의 공정성 정책 : PBE-CC는 다른 congestion control 알고리즘과 wireless link capacity를 공정히 공유<li>propagation delay가 다른 PBE-CC flow : wireless capacity를 공정하게 공유<ul><li>2가지 이유 존재<ul><li>PBE-CC의 design<li>base station의 buffer 구조</ul><li>PBE-CC : fair-share를 명시적으로 계산<ul><li>다른 congestion control 알고리즘 : AMID 방식 채택<ul><li><i>additive increase multiplicative decrease</i><ul><li>packet loss가 없다면 1개씩 window 증가<li>loss가 있다면 절반으로 window를 줄여버림<li><strong>나중에 들어온 user가 있더라도 시간이 흐르면 평형을 이룰수 있게됨</strong></ul><li>additive increase 단계 : 적은 propagation delay를 가진 sender가 window size를 더 빠르게 증가시킴 -&gt; <strong>불공평 야기</strong></ul></ul><li>base station : 모든 user에게 분리된 buffer 제공<ul><li>large $RT_{prop}$이 bottleneck을 지배하는 것을 방지<li>large $RT_{prop}$의 BBR 연결 : large BDP를 계산<ul><li>network 내에 상당한 양의 packet을 흘려보냄 -&gt; bottleneck queue에 저장될 것<li>small $RT_{prop}$을 가진 다른 BBR flow에서의 send rate를 낮추는 결과 초래 -&gt; 적은 수의 packet이 돌아다닐 것</ul><li>cellular base station의 분리된 buffer들 : wireless link를 공유하는 다른 flow들로부터 <strong>packet을 분리, 고립</strong> -&gt; unfair 방지</ul></ul></ul><h2 id="implementation">Implementation</h2><ul><li>control channel의 모든 control message를 decode해야함 -&gt; 전화 내부의 휴대 펌웨어를 customize해야함<ul><li>현재 cellular 펌웨어의 소스코드 : cellular 장비 제조업체의 것 -&gt; 접근 불가능<li><u>펌웨어 수정할 필요 없이 control message를 decode하는 오픈소스 congestion control prototype 플랫폼 구축</u></ul><li>플랫폼 주요 구성요소 : 오픈소스 control channel decoder<ul><li>상용 소프트웨어 정의 radio ( USRP ) -&gt; RF front-end로 사용<ul><li>cellular wireless 신호를 수집</ul><li>PC -&gt; host로 사용<ul><li>수집한 신호에서 control 메세지를 decode</ul></ul><li><strong>다중 병렬 control channel decoder</strong> : 각각 1개의 cell에서 신호를 decode<ul><li>여러 cell에 대해 동시에 작동</ul><li>message fusion module : subframe index에 따라 여러 decoder의 decode control message를 정렬<ul><li>정렬한 message는 congestion control module로 전달</ul><li>cellular control channel decoder : 3300줄의 C언어로 구현 ( 재사용 코드 제외 )<ul><li>재사용 코드도 사용했음 ( 오픈소스 LTE 라이브러리 )<ul><li>wireless channel estimator<li>demodulator ( 복조기 : 변조된 반송파로부터 정보 내용을 복구하기 위함 )<li>convolutional decoder ( 에러 방지를 위한 정정코드를 이용하여 에러를 정정하는 프로그램 )</ul><li>각 decoder : subframe control channel 내에서 <u>가능한 모든 message position</u>을 검색<ul><li>가능한 모든 message format를 시도하여 올바른 message 도출<li>control channel을 decode함</ul><li>병렬 decode 구조 구현 (멀티스레드 구조 사용) : 1개의 PC가 여러 cell의 control channel을 동시에 decode<ul><li>6코어 PC가 각 코어 CPU 사용량을 40% 미만으로 유지하며 6개의 cell을 decode할 수 있었음</ul></ul><li>PBE-CC congestion control 알고리즘의 <strong>user space 기반, UDP 기반 prototype</strong> 구현 : 874줄의 C++로 구현 ( mobile client, sender side )<ul><li>client side PBE-CC module : decode된 control message를 입력으로 받음<ul><li>휴대전화로 sender와 통신<li>data packet을 수신할 때, <u>one-way packet propagation delay</u> $D_{prop}$을 추정하고, 추정 capacity를 반환<ul><li>2개의 1500byte packet을 보내는 interval을 이용하여 capacity 계산, <strong>32bit 정수</strong>로 나타냄</ul><li>현재 bottleneck state를 식별<ul><li>ACK의 1bit를 통해 sender에게 알림</ul></ul><li>server : ACK을 받으면 send rate를 ACK 안에 있는 capacity로 설정<ul><li>수신된 모든 ACK에 대하여 $RT_{prop}$과 $BtlBw$ update -&gt; bottleneck 위치가 변경될 때마다 <strong>즉시 cellular 맞춤형 BBR로 전환할 수 있음</strong></ul></ul></ul><h2 id="evaluation">Evaluation</h2><ul><li>상용 cellular network에서의 PBE-CC의 성능 평가<li>기존 end-to-end congestion control 알고리즘과 비교</ul><h3 id="methodology">Methodology</h3><h5 id="content-sender">Content sender</h5><ul><li>Amazon AWS 서버를 사용 ( PBE-CC sender로 사용 )<li>PBE-CC : 상당히 다른 RTT를 사용<ul><li>성능 테스트를 위해 3대의 미국 서버, 1대의 싱가포르 서버를 사용</ul></ul><h5 id="mobile-client">Mobile client</h5><ul><li>구성요소<ul><li>신호 수집을 위한 다중 USRP<ul><li>USRP X310, B210</ul><li>control channel decoding을 위한 host PC<ul><li>Dell OptiPlex 7060<ul><li>Intel Core i7-8700 CPU<li>16GB RAM<li>Ubuntu 16.04</ul></ul><li>무선 통신을 위한 휴대전화<ul><li>하드웨어 상에서 carrier aggregation을 지원하는 휴대폰을 사용<ul><li>Xiaomi MIX3 // 2 cell<li>Redmi 8 // 1 cell<li>Samsung S8 // 3 cell</ul><li>cellular network : 모든 휴대폰에 동일한 primary cell을 구성<ul><li>각 전화기에 대해서는 서로 다른 수의 aggregated cell을 구성했음</ul></ul></ul></ul><h5 id="congestion-control-algorithms-to-compare">Congestion control algorithms to compare</h5><ul><li>7개의 알고리즘과 성능을 비교할 것<ul><li>cellular network를 위해 설계된 알고리즘<ul><li>Sprout<li>Verus</ul><li>이미 Linux 커널에 포함된 알고리즘<ul><li>BBR<li>CUBIC</ul><li>최근 고안된 알고리즘<ul><li>Copa<li>PCC<li>PCC-Vivace</ul></ul><li>Pantheon 이용 : campus를 커버하는 상업용 cellular network에서 <u>위의 모든 알고리즘을 테스트</u></ul><h3 id="micro-benchmark-cell-status">Micro-benchmark: Cell status</h3><ul><li><p>micro benchmark 수행하여 cell tower의 2가지 중요한 통계를 제시</p><ul><li>매 시간 cell tower와 통신하는 user의 수<li>user의 wireless physical data rate의 distribution</ul><li><p>control channel decoder : 2개의 base station ( 20MHz, 10MHz )이 전송하는 control message를 decode</p><li><p>실험 수행 시간 : 24시간</p><ul><li><p>1시간마다 active user의 수를 계산</p><li><p>peak time인 12~20시에 user 수가 많이 감지됨</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">시간당 user 수<th style="text-align: center">20MHz<th style="text-align: center">10MHz<tbody><tr><td style="text-align: center">최대 user 수<td style="text-align: center">233<td style="text-align: center">135<tr><td style="text-align: center">최소 user 수<td style="text-align: center">13<td style="text-align: center">0<tr><td style="text-align: center">평균 user 수<td style="text-align: center">181<td style="text-align: center">97</table></div><ul><li>0~3시에는 10MHz cell이 운영자에 의해 꺼졌고, 0명의 user가 관측됨</ul></ul><li><p>user들의 wireless physical data rate의 distribution 나타냄</p><ul><li>user들의 data rate는 다양했음 // <strong>대부분 rate가 낮았음</strong><ul><li>각각 77.4% (10MHz), 71.9% (20MHz) 의 user들이 최대 달성가능 data rate (1.8Mbits/s/PRB) 의 <u>절반도 못 미치는 rate</u>를 보임</ul></ul></ul><h3 id="end-to-end-delay-and-throughput">End-to-end Delay and Throughput</h3><ul><li>상용 cellular network 상에서 달성된 <u>PBE-CC의 delay와 throughput 성능</u>을 조사할 것</ul><h4 id="performance-of-stationary-cellular-links">Performance of Stationary Cellular Links</h4><ul><li>고정 cellular link에서의 PBE-CC 성능 조사<ul><li>server &lt;-&gt; 고정된 mobile user간의 연결 생성<li>sender가 20초 동안 연결된 user에게 전송<ul><li>throughput, packet delay, 각 flow의 도착 시간 등을 기록</ul><li>sender가 사용할 수 있도록 알고리즘 수정, 8개의 알고리즘 테스트</ul><li>고려사항<ul><li>알고리즘을 테스트할 때마다 cellular network capacity가 달라짐<ul><li>공정한 throughput 테스트를 위해 각 알고리즘마다 5번 반복</ul><li>서로 다른 수의 aggregated cell의 성능을 측정해야함<ul><li>서로 다른 휴대전화를 사용</ul><li>실내 / 실외 상황에서의 변화 관측해야함<li>cell이 busy / idle인 상태를 측정해야함<ul><li>day, late night 시간대에 따로 측정</ul></ul><li>총 40개 지점, 실내 / 실외, (1, 2, 3) aggregated cell, busy / idle 시간대에서 가능한 <strong>모든 경우의 수</strong>를 테스트</ul><h5 id="comparison-among-high-throughput-algorithm">Comparison among high-throughput algorithm</h5><ul><li>더 높은 throughput을 달성하는 알고리즘<ul><li>PBE-CC<li>BBR<li>CUBIC<li>Verus</ul><li>측정 목록<ul><li>평균 throughput<li>95th percentile one-way delay</ul><li>PBE-CC : <u>가장 높은 throughput, 매우 적은 latency</u><ul><li><p>다른 알고리즘과의 비교</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">알고리즘<th style="text-align: center">throughput<th style="text-align: center">delay reduction (95th)<tbody><tr><td style="text-align: center">BBR<td style="text-align: center">1.06x<td style="text-align: center">1.8x<tr><td style="text-align: center">Verus<td style="text-align: center">1.6x<td style="text-align: center">3.6x<tr><td style="text-align: center">CUBIC<td style="text-align: center">2.3x<td style="text-align: center">1.8x</table></div></ul></ul><h5 id="detailed-comparison-among-eight-algorithms">Detailed comparison among eight algorithms</h5><ul><li><p>6개의 대표적인 위치 선택</p><li><p>(10th, 25th, 50th, 75th, 90th) throughput (100ms 간격), delay를 기록</p><li><p>3가지 관측결과</p><ul><li><p>PBE-CC : 평균 throughput이 높지만, 어느정도 <u>높은 throughput variance</u>를 지님</p><ul><li><p>다양한 wireless channel capacity에 따라 send rate를 일치시킬 수 있기 때문</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">알고리즘<th style="text-align: center">throughput<th style="text-align: center">delay<th style="text-align: center">비고<tbody><tr><td style="text-align: center">BBR<td style="text-align: center">비슷함<td style="text-align: center">더 높음<td style="text-align: center"> <tr><td style="text-align: center">Verus<td style="text-align: center">더 높음<td style="text-align: center">매우 김<td style="text-align: center">cellular network를 위해 만들어진 알고리즘<tr><td style="text-align: center">CUBIC<td style="text-align: center">높음/낮음<td style="text-align: center">높음/낮음<td style="text-align: center">예측 어려움<tr><td style="text-align: center">다른 알고리즘<td style="text-align: center">더 낮음<td style="text-align: center"> <td style="text-align: center">Copa, PCC, PCC-Vivace, Sprout</table></div><li><p>추가 throughput을 달성하기 위해 cellular network <strong>secondary cell을 활성화</strong>하도록 trigger한 위치의 수를 기록</p><ul><li>PBE-CC : 모든 위치에서 carrier aggregation 실행<li>BBR, Verus : 5/6 이상의 위치에서 실행<li>CUBIC : 1/2 이하의 위치에서 실행<li>이외의 알고리즘 : 1/6 이하의 위치에서 실행<ul><li>보수적인 send rate 사용<li>network가 carrier aggregation을 비활성화<li>available wireless capacity 활용도 상당히 저하</ul></ul></ul><li><p>PBE-CC : 낮은 delay, 낮은 delay variance</p><ul><li>BBR, Verus : 상대적으로 높은 throughput, <u>더 높은 delay</u><li>throughput이 낮은 4개의 알고리즘 : latency가 더 낮음<li>delay gap이 발생하는 이유 : <strong>cellular 재전송</strong><ul><li>throughput이 높다 -&gt; TB error rate의 증가 -&gt; 재전송 증가<li>throughput이 높은 체계에서는 packet이 8ms 배수의 재전송 delay 유발</ul></ul><li><p>PBE-CC : cell이 idle 상태라면 <u>throughput, delay 모두 낮은 variance를 보임</u></p><ul><li>traffic, mobility 경쟁이 없다면, idle cell의 static user에게 wireless capacity가 안정화<li>capacity를 정확히 추정할 수 있게 되고, <u>안정적인 throughput, delay</u>를 달성</ul></ul></ul><h5 id="alternation-between-states">Alternation between states</h5><ul><li>평균적으로, 18% (25 busy link), 4% (15 idle link) 의 시간동안 internet bottleneck 상태로써 동작<ul><li>cellular network의 bottleneck은 대부분 <strong>wireless link의 bottleneck</strong>이라는 가설을 입증</ul></ul><h4 id="performance-under-mobility">Performance under Mobility</h4><ul><li><p>cellular wireless capacity 변동 주요 원인 : user mobility에 따른 <u>wireless channel 품질 변화</u></p><li><p>mobility에 따른 PBE-CC의 성능 테스트</p><ul><li><p>cell이 대부분 idle 상태인 야간에 실험 개시</p><ul><li>다른 random 경쟁자의 capacity 변동을 최소화</ul><li><p>실험 예시</p><ul><li>처음 13초 : -85dBm RSSI가 있는 위치에 휴대전화 설치<li>다음 13초 : -105dBm RSSI가 있는 위치로 휴대전화 옮김<li>다음 4초 정도 : 아까보다 빠른 속도로 원래 위치 (-85dBm RSSI) 로 휴대전화 복귀<li>다음 10초 : 원위치에 대기<li><strong>총 40초의 test case</strong> 생성, 테스트마다 이를 반복</ul><li><p>실험 결과</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">알고리즘<th style="text-align: center">throughput<th style="text-align: center">delay (95th)<th style="text-align: center">비고<tbody><tr><td style="text-align: center">PBE-CC<td style="text-align: center">55Mbit/s<td style="text-align: center">64ms<td style="text-align: center"> <tr><td style="text-align: center">BBR<td style="text-align: center">55Mbit/s<td style="text-align: center">156ms<td style="text-align: center"> <tr><td style="text-align: center">CUBIC<td style="text-align: center">38Mbit/s<td style="text-align: center">296ms<td style="text-align: center"> <tr><td style="text-align: center">Verus<td style="text-align: center">41Mbit/s<td style="text-align: center">467ms<td style="text-align: center"> <tr><td style="text-align: center">이외의 알고리즘<td style="text-align: center">low -&gt; wireless capacity의 활용도 낮음<td style="text-align: center"> <td style="text-align: center">PCC, PCC-Vivace, Sprout, Copa</table></div><ul><li>4개의 알고리즘은 wireless capacity의 활용도가 낮고, <u>mobility가 packet delay에 영향을 미침</u></ul><li><p>추가 실험 예시</p><ul><li>40초의 testcase를 20개의 2초 간격으로 나눔<li>각 간격의 throughput 중앙값, delay 중앙값을 표시<li>대상 : PBE-CC, BBR</ul><li><p>추가 실험 결과</p><ul><li>PBE-CC : mobility에 의한 신호 강도의 변화에 따라 <strong>send rate가 정확히 감소하거나 증가</strong><ul><li>network에 buffering이 거의 나타나지 않음</ul><li>BBR : 부정확한 end-to-end capacity 추정<ul><li>신호 강도 변화에 <strong>과민반응</strong> -&gt; send rate가 과도하게 변화<li>throughput 감소 / 과도한 packet delay</ul></ul></ul></ul><h4 id="performance-under-controlled-competition">Performance under Controlled Competition</h4><ul><li><p>network capacity 변화 원인 : 제한된 wireless capacity를 위한 user들간의 경쟁</p><li><p>실험 환경</p><ul><li>제어 가능한 on-off 경쟁 traffic<ul><li>경쟁에 의한 시간 변동 wireless bandwidth 할당을 추적하는 PBE-CC의 기능 시연</ul><li>Redmi 8 전화기 사용, 40초 동안 PBE-CC flow 실행<li>매 8초마다 4초 동안의 동시 flow 실행<ul><li>AWS server로부터의 60Mbit/s의 고정 offered load<li>Xiaomi MIX3를 이용하여 flow 실행</ul><li>야간에 실험 : 다른 user들과의 통제되지 않은 경쟁 가능성을 만듬<li>다른 알고리즘들로 실험 반복</ul><li><p>실험 결과</p><ul><li><p>PBE-CC만이 높은 throughput, 낮은 latency를 지속적으로 달성</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">알고리즘<th style="text-align: center">throughput<th style="text-align: center">Delay (avg)<th style="text-align: center">delay (95th)<tbody><tr><td style="text-align: center">PBE-CC<td style="text-align: center">57Mbit/s<td style="text-align: center">61ms<td style="text-align: center">71ms<tr><td style="text-align: center">CUBIC<td style="text-align: center">58Mbit/s<td style="text-align: center">252ms<td style="text-align: center">416ms<tr><td style="text-align: center">Verus<td style="text-align: center">56Mbit/s<td style="text-align: center">263ms<td style="text-align: center">403ms<tr><td style="text-align: center">BBR<td style="text-align: center">62Mbit/s<td style="text-align: center">147ms<td style="text-align: center">227ms</table></div></ul><li><p>경쟁 traffic에 대한 react를 추가로 입증하는 실험 진행</p><ul><li>200ms 간격마다 troughput 평균, 수신된 모든 packet의 delay를 나타냄<li>MIX3에 의해 traffic에 경쟁이 발생한 부분을 음영으로 표기</ul><li><p>추가 실험 결과</p><ul><li>PBE-CC<ul><li>경쟁자의 진입을 정확히 추적 -&gt; send rate를 지체없이 낮춤<ul><li><strong>packet queuing이 거의 발생하지 않음</strong></ul><li>경쟁 traffic 흐름이 끝남을 탐지 -&gt; idle bandwidth를 즉시 확보<ul><li><strong>throughput을 극대화</strong></ul></ul><li>BBR<ul><li>경쟁 트래픽에 의한 capacity 감소를 바로 감지할 수 없음<ul><li>delay가 상당히 커지는 결과 초래</ul></ul></ul></ul><h4 id="single-device-multiple-connections">Single device multiple connections</h4><ul><li><p>하나의 장치가 서로 다른 원격 서버에 다중연결하는 경우에서의 평가</p><li><p>실험 환경</p><ul><li>MIX3 휴대전화로 2개의 AWS 서버로 각각 40초 동안 동시에 flow 생성<li>각 알고리즘마다 throughput과 delay 측정</ul><li><p>실험 결과</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">알고리즘<th style="text-align: center">throughput<th style="text-align: center">delay<tbody><tr><td style="text-align: center">PBE-CC<td style="text-align: center">26Mbit/s, 28Mbit/s<td style="text-align: center">48ms, 56ms<tr><td style="text-align: center">BBR<td style="text-align: center">10Mbit/s, 35Mbit/s<td style="text-align: center"> </table></div><ul><li>단일 연결에 대한 throughput은 낮을 수도 있음<ul><li><strong>연결 전반에 걸쳐서 공정성 제공</strong></ul></ul></ul><h3 id="fairness">Fairness</h3><ul><li>bottleneck이 cellular wireless link일 때의 PBE-CC의 공정성에 대한 평가</ul><h5 id="methodology-1">Methodology</h5><ul><li><p>base station resource 할당 알고리즘 + 공정성 정책을 알지 못함 -&gt; 시뮬레이션 기반 실험은 실제 cellular network에서의 동작을 예측할 수 없음</p><ul><li>cellular deployment에서 직접 공정성 평가</ul><li><p>background traffic의 영향을 제거해야함</p><ul><li>cell이 idle 상태일 밤에 실험</ul><li><p>실험 환경</p><ul><li><p>3개의 휴대전화를 경쟁 user로써 사용 // 각각 AWS server에 연결</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">휴대전화 기종<th style="text-align: center">연결 시작시간<th style="text-align: center">연결 종료시간<tbody><tr><td style="text-align: center">S8<td style="text-align: center">0s<td style="text-align: center">60s<tr><td style="text-align: center">Redmi 8<td style="text-align: center">10s<td style="text-align: center">50s<tr><td style="text-align: center">MIX3<td style="text-align: center">20s<td style="text-align: center">40s</table></div><li><p>3대의 전화기는 primary cell을 공유</p><ul><li>secondary cell, 구성된 경우 tertiary cell은 다른 것을 사용<li><strong>1.94GHz의 primary cell이 공유 bottleneck</strong></ul><li><p>3개의 연결이 동시에 진행중일 때, primary cell에 의해 각 user에게 할당된 PRB를 기록</p><ul><li>fair-share를 달성한 경우, <u>같은 양의 PRB를 할당받았을 것</u></ul></ul></ul><h4 id="multi-user-fairness">Multi-user fairness</h4><ul><li>비슷한 propagation delay를 가진 PBE-CC user들에 대한 공정성 조사<li>실험 환경<ul><li>3개의 미국 AWS 서버 설정<li>3개의 휴대전화를 사용하여 각각 연결 시작<li>primary cell에 의해 할당된 PRB를 기록</ul><li>실험 결과<ul><li>PBE-CC : <strong>bottleneck bandwidth의 fair-share로 빠르게 수렴</strong><li>Jain의 공정성 지수<ul><li>2개의 동시 flow : 99.97%<li>3개의 동시 flow : 98.73%</ul></ul><li>연결된 모든 user가 cellular network를 사용하지 못하게 할 수는 없음<ul><li>unknown user가 생성한 light background traffic을 관찰<li>PBE-CC : background user와도 fair-share 작동</ul></ul><h4 id="rtt-fairness">RTT fairness</h4><ul><li>다른 propagation delay를 가진 다중 flow 사이의 wireless capacity의 fair-share를 보장하는지에 대한 조사<li>실험 환경<ul><li>3대의 휴대전화 사용<li>3대의 AWS 서버에 각각 연결<ul><li>1개의 싱가포르 서버 (평균 RTT = 297ms)<li>2대의 미국 서버 (평균 RTT = 52ms, 64ms)</ul><li>각 연결에 대해 할당된 primary cell의 PRB를 기록</ul><li>실험 결과<ul><li>delay 차이가 큰 3개의 PBE-CC flow 모두 <strong>비슷한 할당 bandwidth</strong>를 얻음<li>Jain의 공정성 지수<ul><li>2개의 동시 flow : 99.74%<li>3개의 동시 flow : 99.45%</ul></ul></ul><h4 id="tcp-friendliness">TCP friendliness</h4><ul><li>새로운 congestion control 체계에 요구되는 사항 : <u>기존의 알고리즘과 bandwidth를 공정하게 나눌 수 있어야 함</u><li>실험 예시<ul><li>2 PBE-CC user, 1 BBR user의 상황<li>2 PBE-CC user, 1 CUBIC user의 상황<li>각 상황에서의 할당 PRB를 기록</ul><li>실험 결과<ul><li>PBE-CC : 다른 체계의 flow들과 <strong>bottleneck bandwidth를 동일하게 공유</strong><li>Jain의 공정성 지수<ul><li>BBR과의 2, 3 동시 flow : 99.96%, 98.52%<li>CUBIC과의 2, 3 동시 flow : 99.95%, 98.34%</ul></ul><li>base station의 공정성 정책 : 한 user가 모든 bandwidth를 점유할 수는 없음<li>CUBIC, BBR : send rate를 급격히 올릴 수 있음<ul><li>base station : 얻을 수 있는 bandwidth 제한 -&gt; 다른 flow와 강제적으로 공유하도록 함</ul></ul><h2 id="discussion">Discussion</h2><h5 id="power-consumption">Power consumption</h5><ul><li>mobile 장치 : 연결중이면 radio를 켜고 control channel을 decode하여 <u>각 subframe에 해당 data가 있는지 여부를 검사해야함</u><ul><li>PBE-CC : 현재 필요 이상의 시간동안 radio를 켜지 않음<ul><li><strong>추가 전력 cost가 발생하지 않음</strong></ul><li>작은 computational overhead 도입<ul><li>전송되지 않은 control message를 decode해야할 수도 있음<li>개수는 매우 적음 ( 95% 이상의 subframe 내부에는 4개 미만의 control message가 있음 )<li>길이도 매우 짧음 ( 70bit 미만 )</ul></ul></ul><h5 id="packet-buffering">Packet buffering</h5><ul><li>PBE-CC : Kleinrock TCP 운영 지점에서 실행됨<ul><li>buffering, delay를 최소화하는 지점</ul><li>실제로는 <u>base station의 어느 정도의 byte를 buffering해두는 것이 좋음</u><ul><li>send rate를 조정하는데 delay가 약간 듬<li>connection throughput 증가를 즉시 활용할 수 있음<li>( congestion control에는 최소한의 RTT delay는 있음 )</ul><li>미래 계획 ( PBE-CC 확장 )<ul><li>sender/app 등이 network 내부의 buffering을 적응적으로 조정할 수 있도록 할 예정<li><u>증가된 throughput을 위해 증가한 delay를 상쇄할 계획</u></ul></ul><h5 id="fairness-policy">Fairness policy</h5><ul><li>PBE-CC : 연결 시작 상태에서는 모든 active user와 idle bandwidth를 fair-share<li>미래 계획 ( 공정성 정책 통합목적 )<ul><li>낮은 physical data rate를 가진 user가 더 많은 양의 bandwidth를 잡을 수 있도록</ul><li>PBE-CC : 임의의 공정성 정책에 적응 -&gt; steady state에서 평형 달성</ul><h5 id="misreported-congestion-feedback">Misreported congestion feedback</h5><ul><li>PBE-CC : mobile user가 추정 capacity를 server에 보고<ul><li>문제점 : 악의적인 user가 network가 지원할 수 있는 양보다 많은 data rate를 보고할 수 있음 -&gt; <strong>network에 엄청난 수의 data가 발생</strong></ul><li>미래 계획 ( 악의적인 user를 감지하는 기능 추가 )<ul><li>server-side에서 실행되는 <u>BBR과 유사한 throughput estimator 구현 </u><ul><li>mobile user의 개입 없이 <strong>packet의 전송과 ACK의 timestamp만으로 현재 throughput 추정</strong></ul><li>PBE-CC : user가 제시하는 capacity와 계산한 throughput을 비교<ul><li>더 높은 capacity를 제시하는 user를 악의적인 user로 간주</ul></ul></ul><h2 id="conclusion">Conclusion</h2><ul><li>PBE-CC : mobile user측 wireless physical-layer capacity 추정을 설계에 통합한 최초의 end-to-end congestion control 알고리즘<ul><li>4G, 5G wireless network <strong>multicell 설계</strong>에 매우 중요</ul><li>multi-location, mobility, 다양한 background traffic level, 다양한 RTT를 특징으로 하는 <u>철저한 성능 평가</u> 실행<ul><li>PBE-CC : latency와 throughput 모두에서 선두적인 알고리즘을 능가</ul><li>즉시 배포 가능<ul><li>content server와 mobile user만 수정하면 사용할 수 있음<li>수정 작업은 어떠한 윤리적 문제도 제기하지 않음</ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/imes/'>IMES</a>, <a href='/categories/paper/'>paper</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/imes/" class="post-tag no-text-decoration" >IMES</a> <a href="/tags/paper/" class="post-tag no-text-decoration" >paper</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement - Joe2357&url=https://joe2357.github.io/posts/PBE-CC/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement - Joe2357&u=https://joe2357.github.io/posts/PBE-CC/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement - Joe2357&url=https://joe2357.github.io/posts/PBE-CC/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/SCPC-2021-Round-1/">SCPC 2021 1라운드 후기</a><li><a href="/posts/SCPC-2020-Round-1/">SCPC 2020 1라운드 후기</a><li><a href="/posts/728/">Codeforces Round #728 (Div. 2) 후기</a><li><a href="/posts/727/">Codeforces Round #727 (Div. 2) 후기</a><li><a href="/posts/726/">Codeforces Round #726 (Div. 2) 후기</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ps/">PS</a> <a class="post-tag" href="/tags/review/">Review</a> <a class="post-tag" href="/tags/codeforces/">Codeforces</a> <a class="post-tag" href="/tags/imes/">IMES</a> <a class="post-tag" href="/tags/paper/">paper</a> <a class="post-tag" href="/tags/scpc/">SCPC</a> <a class="post-tag" href="/tags/ds/">DS</a> <a class="post-tag" href="/tags/storage/">Storage</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/WebRTC/"><div class="card-body"> <span class="timeago small" > May 26 <i class="unloaded">2021-05-26T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Audio and Video Mixing Method to Enhance WebRTC</h3><div class="text-muted small"><p> 2021 / 7 / 21 IMES 세미나 Abstract WebRTC : 웹브라우저에 JavaScript API를 호출함으로써 P2P 라이브 스트리밍 제공 소수의 peer로 제한되는 프로토콜 ( 다중 peer들의 real-time 스트림을 mix하기 힘듬 / mix된 스트림을 많은 수의 audience에게 분배할 수...</p></div></div></a></div><div class="card"> <a href="/posts/MPBond/"><div class="card-body"> <span class="timeago small" > Jan 21 <i class="unloaded">2021-01-21T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MPBond: Efficient Network-level Collaboration Among Personal Mobile Devices</h3><div class="text-muted small"><p> 2021 / 2 / 17 IMES 세미나 Abstract MPBond : 여러 개인 mobile device가 공동으로 internet에서 content를 가져올 수 있도록 하는 효율적인 system 스마트워치 : data downloading을 통해 페어링된 스마트폰 지원 MPTCP ( Multipath...</p></div></div></a></div><div class="card"> <a href="/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/"><div class="card-body"> <span class="timeago small" > Feb 17 <i class="unloaded">2021-02-17T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Edge Assisted Real-time Object Detection for Mobile Augmented Reality</h3><div class="text-muted small"><p> 2021 / 3 / 18 IMES 세미나 Abstract 대부분의 AR/MR : 주변 환경의 3D 형상을 이해할 수 있음 단점 : 현실의 복잡한 물체 탐지, 분류하는 능력 부족 해결 방법 : CNN에서 기능을 활성화할 수 있음 단점 : mobile device에서 대규모 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/SCPC-2020-Round-1/" class="btn btn-outline-primary" prompt="Older"><p>SCPC 2020 1라운드 후기</p></a> <a href="/posts/695/" class="btn btn-outline-primary" prompt="Newer"><p>Codeforces Round #695 (Div. 2) 후기</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/username">Joe2357</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/ps/">PS</a> <a class="post-tag" href="/tags/review/">Review</a> <a class="post-tag" href="/tags/codeforces/">Codeforces</a> <a class="post-tag" href="/tags/imes/">IMES</a> <a class="post-tag" href="/tags/paper/">paper</a> <a class="post-tag" href="/tags/scpc/">SCPC</a> <a class="post-tag" href="/tags/ds/">DS</a> <a class="post-tag" href="/tags/storage/">Storage</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://joe2357.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
