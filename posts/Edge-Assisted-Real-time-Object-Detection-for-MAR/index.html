<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Edge Assisted Real-time Object Detection for Mobile Augmented Reality" /><meta name="author" content="Joe2357" /><meta property="og:locale" content="en_US" /><meta name="description" content="2021 / 3 / 18 IMES 세미나" /><meta property="og:description" content="2021 / 3 / 18 IMES 세미나" /><link rel="canonical" href="https://joe2357.github.io/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/" /><meta property="og:url" content="https://joe2357.github.io/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/" /><meta property="og:site_name" content="Joe2357" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-02-17T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Edge Assisted Real-time Object Detection for Mobile Augmented Reality" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Joe2357" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Joe2357"},"headline":"Edge Assisted Real-time Object Detection for Mobile Augmented Reality","dateModified":"2021-03-28T02:56:22+09:00","datePublished":"2021-02-17T00:00:00+09:00","description":"2021 / 3 / 18 IMES 세미나","url":"https://joe2357.github.io/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://joe2357.github.io/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/"},"@context":"https://schema.org"}</script><title>Edge Assisted Real-time Object Detection for Mobile Augmented Reality | Joe2357</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/profile.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Joe2357</a></div><div class="site-subtitle font-italic">공부하는 블로그</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/Joe2357" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['deviljoe996','gachon.ac.kr'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Edge Assisted Real-time Object Detection for Mobile Augmented Reality</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Edge Assisted Real-time Object Detection for Mobile Augmented Reality</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Feb 17, 2021, 12:00 AM +0900" > Feb 17 <i class="unloaded">2021-02-17T00:00:00+09:00</i> </span> by <span class="author"> Joe2357 </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Mar 28, 2021, 2:56 AM +0900" > Mar 28 <i class="unloaded">2021-03-28T02:56:22+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="8558 words">47 min</span></div></div><div class="post-content"><blockquote><p>2021 / 3 / 18 IMES 세미나</p></blockquote><h2 id="abstract">Abstract</h2><ul><li>대부분의 AR/MR : 주변 환경의 3D 형상을 이해할 수 있음<ul><li>단점 : 현실의 <u>복잡한 물체 탐지, 분류하는 능력 부족</u><li>해결 방법 : CNN에서 기능을 활성화할 수 있음<ul><li>단점 : mobile device에서 <u>대규모 network 실행은 어려움</u></ul><li>edge / cloud에 object detection을 offload하는 것도 어려움<ul><li>원인 : 높은 accuracy/낮은 E2E latency에 대한 엄격한 요구사항</ul><li>기존의 offloading 기술의 긴 latency : <strong>detection accuracy를 상당히 낮출 수 있음</strong><ul><li>원인 : user view의 변화</ul></ul><li>높은 accuracy의 object detection를 할 수 있는 system 설계<ul><li>환경 : 60fps / 상용 AR/MR system<li>특징<ul><li>낮은 latency의 offloading 기술 사용<li>rendering pipeline을 offload pipeline으로부터 분리<li>빠른 object tracking method 사용<ul><li>detection accuracy 유지</ul></ul><li>결과<ul><li>object detection / human keypoint detection task에서의 accuracy : 20.2% -&gt; 34.8%<li>AR device에서의 object tracking의 latency : 2.24ms</ul><li>결론<ul><li>다음 frame에 대한 가상 element를 rendering하기 위한 <u>시간과 resource를 더 많이 남겨놓을 수 있음</u><li>더 높은 quality의 AR/MR 체험을 가능하게 함</ul></ul></ul><h2 id="introduction">Introduction</h2><ul><li>AR, Mixed Reality : 전례없는 몰입 경험 제공할 것 ( 교육 / 오락 / 의료 분야 등 )<ul><li>카메라를 통한 주변환경 이해<li>사용자의 시야에 가상 오버레이 렌더링</ul><li>MR : system이 실제 다양한 object와 instance를 이해하고 렌더링해야함<ul><li><strong>더 많은 computing resource가 필요</strong><li>미래시장이 좋음 ( 2021년에 9900만 기기 / 1080억 달러 )</ul><li>기존 모바일 AR solution ( ARKit / ARCore ) : 스마트폰에서의 표면 감지 / 객체 고정 기능 지원<ul><li>보다 정교한 AR 헤드셋 ( Microsoft HoloLens / announced Magic Leap One ) : 주변환경의 3D 형상 이해, 60fps에서의 가상 오버레이 렌더링 가능<li>기존 AR 시스템 : 표면 감지는 가능 / <strong>실제 환경에서의 복잡한 물체 감지, 분류 능력 부족</strong><ul><li>많고 새로운 AR / MR app에서의 필수요소<ul><li>자동차 운전자에게 위험요소 알려주는 것<li>사람 위에 피카츄 올려놓는 것</ul></ul></ul><li>위의 기능 : CNN과 함께 enable 가능 ( 객체탐지 task에서의 우수한 성능 )<ul><li>문제점 : 모바일 장치에서 latency가 적도록 대규모 네트워크 실행하는 것은 어려움<ul><li>예시 : tensorflow lite는 한 프레임에서 CNN 모델을 실행하는데 1초 이상 소요</ul></ul><li>edge / cloud로의 객체탐지 offload는 어렵다 ( 높은 정확도, 적은 delay 등의 엄격한 요구사항이 원인 )<ul><li>좋은 품질의 AR 장치 : 객체를 성공적으로 분류해야함 + <u>높은 정확도로 객체를 현지화해야함</u><li>delay가 100ms보다 작더라도 user view의 변화에 의해 정확도가 떨어짐 ( 탐지한 객체의 프레임 위치와 현재 프레임 위치랑 다를 수 있음 )<li>MR 그래픽이 VR의 복잡성에 근접 -&gt; VR app에서 멀미를 일으키지 않을 정도의 delay ( 20ms ) 가 요구될 것<ul><li>기존 AR ( 간단한 annotation만을 렌더링 ) 과의 비교 : 더 많은 가상요소를 더 좋은 품질로 렌더링해야함 -&gt; <strong>객체 탐지 task에서의 latency budget이 적어짐</strong></ul></ul><li>기존 연구들 : 모바일 장치에서의 높은 프레임률 객체 탐지 기능 지원에 초점<ul><li><strong>고품질 AR / MR에서의 latency 요구사항은 고려하지 않음</strong><ul><li>Glimpse : 트리거 프레임을 클라우드 서버로 offload -&gt; 30fps 객체탐지 달성 / 모바일 장치의 나머지 프레임에서의 bounding box를 로컬로 추적<li>DeepDecision : 네트워크 상황에 따라 객체탐지 task를 edge cloud로 보낼지 / local 추론을 실행할 것인지 결정하는 프레임워크 설계<li>위의 방법 : 400ms 이상의 latency 필요 / 대량의 local 계산 필요 -&gt; <strong>고품질 가상 오버레이를 렌더링할 resource가 거의 남지 않음</strong><ul><li><u>움직이는 시나리오에서의 높은 감지 정확도 달성 / 전체 감지 및 렌더링 pipeline을 20ms 미만으로 완료할 수 없음</u></ul></ul></ul><li>위의 문제점을 달성하기 위해, 새로운 시스템 제안 ( <strong>offload 감지 latency 크게 줄이고 기기 내에서 빠른 객체 추적 방법으로 나머지 latency를 숨김</strong> )<ul><li>offloading latency 줄이는 방법 : <em>Dynamic RoI Encoding</em> 기술 / <em>Parallel Streaming and Inference</em> 기술<ul><li>Dynamic RoI Encoding : 마지막 offload된 프레임에서 검출된 <em>관심 영역</em>에 기초하여 전송 latency를 줄이기 위해 각 프레임의 encoding 품질을 조정<ul><li>key innovation : 이전 프레임에서 후보 영역의 잠재적 관심 대상이 존재하는 영역을 식별하는 것<li>객체가 탐지될 가능성이 높은 영역에서 더 높은 encoding 품질 제공 / 다른 영역에서는 더 강한 compression 실행 -&gt; <strong>bandwidth 절약 / latency 줄임</strong></ul><li>Parallel Streaming and Inference : 스트리밍 / 추론 프로세스를 pipeline -&gt; offloading latency 줄이는 목적<ul><li>검출 결과에 영향을 주지 않고 CNN 객체탐지 모델의 slice 기반 추론을 가능하게 하는 <em>Dependency Aware Inference</em> 방식 제안<li>AR 장치에서 렌더링 pipeline과 offloading pipeline을 분리 ( 모든 프레임에 대한 edge cloud로부터의 검출 결과를 기다리지 않음 )<ul><li>빠르고 가벼운 객체 탐지 방법 사용 ( encode된 비디오 프레임에서 추출되거나 edge cloud에서, 이전 프레임에서 처리된 프레임에서 캐시된 객체탐지 결과를 기반으로 한 모션벡터 기반 )<li>모션이 있는 경우 현재 프레임에서의 bounding box나 key point를 조정</ul></ul></ul><li>낮은 offload 지연시간 -&gt; <strong>정확한 객체탐지 결과 제공 / AR이 고품질 가상 overlay를 렌더링할 시간과 resource 남겨둘 수 있음</strong><ul><li><em>Adaptive Offloading</em> 기술 도입 : 이전의 offload 프레임과 비교하여 현재 프레임의 변경사항을 기반으로, 각 프레임을 edge cloud로 offload할지 결정 -&gt; bandwidth / energy 효율적</ul></ul><li>이 시스템은 현재 AR/MR 시스템, 60fps 환경 ( 객체 감지 / 인간 keypoint 감지 task ) 에서 높은 정확도의 객체탐지를 이루어냄<ul><li>상용 AR기기에서 E2E AR 플랫폼 구축 ( 평가목적 )<ul><li>정확도 향상 ( 20.2% ~ 34.8% )<li>탐지 오류 감소 ( 27.0% ~ 38.2% )<li>latency : 2.24ms ( 매우 짧음 ) / AR 장치의 15% 미만의 resource 사용 =&gt; 프레임 사이의 여유시간이 남음<ul><li><strong>고품질 AR/MR 환경을 위한 고품질 가상 element를 렌더링 할 시간이 있음</strong></ul></ul></ul><li>기여<ul><li>객체탐지 task offload의 상태에서, E2E AR 시스템에서 정확도와 latency 필요를 수량화<li>개별 렌더링 / offload pipeline이 포함된 프레임워크 제안<li><em>Dynamic RoI Encoding</em> 기술 설계 : 관심영역을 동적으로 결정하여 offload pipeline 전송 latency / bandwidth 사용을 줄임<li><em>Parallel Streaming and Inference</em> 메소드 개발 : 스트리밍 / 추론 프로세스를 pipeline하여 offload latency를 더욱 줄임<li><em>Motion Vector Based Object Tracking</em> 기술 개발 : 인코딩된 비디오 스트림에서의 내장 motion 벡터에 기반하여 AR장치에서 빠르고 가벼운 객체 추적 달성<li>상용 하드웨어에 E2E system 구현 / 평가 : 제안된 시스템이 정확한 객체탐지를 통해 60fps AR experience 실현할 수 있음을 보임</ul></ul><h2 id="challenges-and-analysis">Challenges and Analysis</h2><ul><li>모바일에서의 정교한 객체탐지는 어려움 ( 모바일에서 계산을 모두 돌리기 힘듬 / bandwidth 사용이 너무 많어 edge로 offload할 수 없음 )<ul><li>간단한 객체탐지 : GPU 사용하는 모바일에서도 500ms의 processing time 소요<li>더 좋은 GPU를 사용하더라도 HD frame에서 50ms의 delay 소요<li>초래하는 문제점<ul><li>60Hz의 모든 frame을 process할 수 없음<li>에너지 소비 문제<li>방열 문제</ul></ul></ul><h5 id="latency-analysis">latency analysis</h5><ul><li><p>객체탐지를 더 강한 edge나 cloud로 offload하는 것은 상당한 latency를 추가함</p><ul><li>탐지 정확도 감소 / AR experience 저하 초래</ul><li><p>전체 latency 모델 생성 \(t_{e2e} = t_{offload}+t_{render}\\ t_{offload}=t_{stream}+t_{infer}+t_{trans\_back}\\ t_{stream}=t_{encode}+t_{trans}+t_{decode}\)</p><ul><li>과정<ul><li>AR device -&gt; Edge : frame $n$개를 capture<li>Edge : $n$개의 frame을 받고, 그것을 inference<li>Edge -&gt; AR device : 결과 전송<li>AR device : screen에 결과를 렌더링</ul><li>$t_{stream}$을 줄이는 방법 : 몇개의 raw frame을 압축 ( H.264 video )</ul><li><p>기존 AR system으로는 1280x720 resolution / 60fps에서의 높은 객체탐지 정확도를 실현하는 것은 어려움</p></ul><h5 id="detection-accuracy-metrics">detection accuracy metrics</h5><ul><li>물체 분류 / localization에서의 객체 정확도 평가 목적 : 각 경계 상자의 IoU, ground truth를 정확도 지표로써 계산<ul><li>IoU : 교집합 영역 넓이 / 합집합 영역 넓이 ( bounding box의 감지 비율이 0.75 미만이라면 실패했다고 판단 )</ul><li><strong>적은 latency의 객체탐지는 높은 정확도와 직결됨</strong><ul><li>$t_{offload}의 높은 latency가 정확도가 낮아지는 원인이 됨 ( user view의 변화 / user motion의 변화 / scene motion의 변화 )</ul><li>상용 인프라에서 객체탐지 latency를 낮추는 것이 어려움<ul><li>모든 backbone CNN 네트워크에서 최소 10ms의 객체탐지 latency가 요구됨</ul><li>영상의 bitrate가 높아질수록 offload할 latency가 높아짐<ul><li>bitrate를 낮추면 frame의 손실이 발생</ul><li>$t_{infer} + t_{trans}$ latency가 한 frame의 display time을 넘어섬<ul><li>인코딩 bitrate의 resolution을 낮추면 latency는 낮출 수 있지만 정확도가 감소<li>적어도 90%의 정확도를 위해서는 50Mbps의 인코딩 bitrate가 필요<li>해상도를 낮추는 것 또한 정확도 감소</ul><li><strong>60fps 기존 AR 시스템에서 높은 정확도를 나타내는 것은 어렵다</strong><ul><li>품질이 좋지 않은 객체를 렌더링하는 결과를 초래할 것</ul></ul><h2 id="system-architecture">System Architecture</h2><ul><li>한계 돌파하기 위해, AR 플랫폼에서 <u>높은 정확도의 객체탐지 + 적은 랜더링 오버헤드 시스템 고안</u><ul><li>low latency offloading 기술로 detection latency를 낮춤<li>on-device fast object tracking method로 잔여 latency를 숨김</ul><li>시스템 구조 : 2개의 시스템의 무선 연결구조<ul><li>local tracking + rendering system ( 모바일 )<li>pipelined object detection system ( edge )</ul><li>객체 탐지 task에서의 latency를 줄이기 위해 rendering process / CNN offloading process를 2개의 pipeline으로 분리<ul><li>local rendering pipeline은 scene을 추적하고 가상 overlay 렌더링 시작<li>결과를 받으면 탐지 결과를 포함시킴</ul><li>2개의 pipeline에서 <em>Dynamic RoI Encoding technique</em> 기술 사용<ul><li>2가지 기능<ul><li>CNN offloading pipeline : raw frame을 압축<li>tracking and rendering pipeline : on-device tracking 모듈에 메타데이터 제공</ul><li><strong>탐지 정확도는 유지하면서 edge cloud의 bandwidth 소모 / 전송 latency 줄일 수 있는 기술</strong><li>이전의 탐지 결과에 따라 <u>가능성이 없는 frame 부분의 인코딩 품질을 낮게, 가능성이 높은 부분의 인코딩 품질은 유지하는 기법</u><li>이후의 video frame에 대한 시공간 상관관계 존재 -&gt; 마지막에 offload된 frame의 intermediate inference 결과를 후보 영역으로 사용<ul><li>높은 인코딩 품질을 유지 / RoI로써 참고되기도 함</ul></ul><li>CNN offloading pipeline에서 <em>Adaptive Offloading</em> + <em>Parallel Streaming and Inference</em> 기술 사용<ul><li><em>Adaptive Offloading</em> 기술 : 이전 frame과의 변화를 기준으로 <u>현재 frame을 edge로 offload할 것인지를 결정하는 기술</u><ul><li>system의 bandwidth와 power 소모를 줄이는 기술<li>macroblock 타입을 재사용하여 변화를 감지</ul><li><em>Parallel Streaming and Inference</em> 실행 : frame이 offload되어야한다고 mark되면 <u>전송, decode, inference를 병렬로 실행하는 method</u><ul><li>모든 frame을 전송받기를 기다리지 않고, 1개의 slice ( frame을 분할한 것 ) 가 들어올 때마다 CNN 객체탐지 task 수행<li><strong>수신 / decode / 객체탐지 기능이 병렬로 실행될 수 있음</strong><li>slice 간의 종속성 문제 존재 -&gt; <em>Dependency Aware Inference</em> 매커니즘 고안<ul><li>각 slice의 수신 이후 계산할 수 있는 충분한 input feature를 가진 각 feature map 상에서 region 결정<li>결정된 region에 존재하는 feature만 계산</ul></ul><li>계산된 결과는 AR 기기에 return<ul><li>cache되어 나중에 사용할 수도 있음</ul></ul><li>tracking and rendering pipeline에서 <em>Motion vector based Object Tracking</em> 기술 사용<ul><li>fast / light-weight 기술<li>viewer / scene의 motion과 이전에 캐시해둔 detection 결과를 조정하기 위함<li>인코딩된 video frame에 내장된 motion vector 활용하여 추가 processing overhead 없이 객체탐지를 가능하게 함<ul><li>기존 객체탐지 기술 : 2개의 frame에서 match되는 image feature point를 찾는 방법 사용</ul><li>더 적은 frame 시간으로도 tracking 가능 / 상당한 정확도 제공 가능<li>device에서 렌더링할 시간과 연산 resource를 남겨놓을 수 있음 ( 16.7ms 미만 )</ul></ul><h2 id="dynamic-roi-encoding">Dynamic RoI Encoding</h2><ul><li>객체탐지 정확도는 유지하면서 offloading pipeline의 transmission latency를 줄이는 기법<ul><li>높은 품질의 frame을 전송하는 것은 큰 bandwidth 소모 발생 -&gt; latency로 이어짐</ul><li>관심 영역에 따라 높은 degree의 frame 압축을 할 것인지 결정<ul><li>가능성이 없다면 압축하여 frame의 크기를 줄임<li>가능성이 높다면 높은 품질로 유지</ul><li>객체탐지 정확도 향상에 도움이 됨<li>핵심 : 관심영역 ( RoI ) region을 식별하는 것<ul><li>이전 frame을 CNN으로 돌려서 생성된 후보영역을 사용</ul><li>기존 RoI 인코딩 기법을 사용 + 새로운 기술을 추가하여 <strong>각 frame의 RoI를 동적으로 결정</strong></ul><h3 id="1-preliminaries">1. Preliminaries</h3><h5 id="roi-encoding">RoI encoding</h5><ul><li>RoI encoding을 만드는 것은 다른 app들에서도 사용되고 있음<ul><li>region을 선택하는 현재 method는 AR 객체탐지 task에 적합하지 않음</ul><li>이미 대부분의 video 인코딩 플랫폼에서 지원됨<ul><li>user가 frame의 macroblock마다 인코딩 품질을 조정할 수 있음</ul><li>선택된 region은 손실이 거의 없는 압축 ( 품질 유지 )<ul><li>선택되지 못한 region ( 배경 등 ) 은 손실이 있는 압축</ul><li>RoI는 사용자의 초점에 맞는 현재 object에 기초할 수 없음</ul><h5 id="object-detection-cnns">object detection CNNs</h5><ul><li>존재하는 다른 네트워크처럼 비슷한 구조를 공유함<ul><li>CNN 네트워크를 이용하여 input image로부터 feature를 추출<li>region 제안 네트워크를 통해 RoI와 가능성을 내부적으로 제안<li>객체 분류를 수행 / 정제</ul><li>CNN 네트워크를 backbone 네트워크라고도 부름<li>대체로 frame에서 가능성이 있는 영역으로 수백개의 region을 생성함</ul><h3 id="2-design">2. Design</h3><ul><li>bandwidth 소모와 전송 latency를 줄이는 것이 목적<li>interal RoI를 image incoder와 link하는 기술<li>이전 frame에서 생성된 candidate RoI를 사용하여 다음 camera frame에서의 인코딩 품질 결정<li>하나의 macroblock으로 RoI를 확장 -&gt; motion의 degree를 약간 수용 / 2개의 frame에 대한 유사성에 대한 이점을 얻을 수 있음<li><strong>object가 발견된 region만을 RoI로 지정하면 안됨</strong><ul><li>새로운 object가 나타날 수 있는 region이 심하게 압축되버릴 수 있음<li>그래도 RoI의 일부로 종종 확인되며, region 제안 네트워크의 출력으로 나오기도 함<ul><li><u>최소 예측 신뢰 임계값 0.02로 필터링하여 RoI를 사용</u></ul></ul><li>현재 frame의 인코딩 품질을 조정하기 위해 선택된 RoI 사용 // QP map 계산<ul><li>QP : ( Quantization Parameter ) : 인코딩 품질<li>QP map : frame의 각 macroblock에 대해 인코딩 품질 정의<ul><li>macroblock이 다른 RoI와 겹치는가를 표시<li>결과를 AR device에 반환 // <strong>이것을 다음 frame의 RoI로 사용</strong><ul><li>RoI는 품질 유지, 이외의 범위는 손실압축</ul></ul></ul></ul><h2 id="parallel-streaming-and-inference">Parallel Streaming and Inference</h2><ul><li>무거운 DNN 연산은 edge cloud로 옮겨 계산 -&gt; 카메라 frame을 전송해야하는 필요성<ul><li>conventional 구조 특성 : <u>모든 frame이 도착해야 객체탐지 process 실행 가능</u><ul><li>deep 신경망이 이웃간의 종속성에 의해 설계되기 때문</ul><li>streaming과 inference 모두에게 상당한 latency가 발생함</ul><li><em>Parallel Streaming and Inference</em> 기술 도입 : 각 frame마다 inference를 수행할 수 있게 함<ul><li><strong>streaming과 inference가 효율적으로 pipelined되고 병렬적으로 수행됨</strong><li>가능한 이유 : <u>각 단계마다 사용하는 resource가 다름</u><ul><li>전송 : 무선 link의 bandwidth<li>decode : edge cloud의 하드웨어 decoders<li>네트워크 inference : GPU / FPGA resource</ul><li>frame마다 실행에서의 challenge : input 간의 종속성<ul><li>이웃 value를 input으로써 받는 신경만 operation에 의함<li><strong>Dependency Aware Infefence</strong> 기술 제안 : 각 layer의 종속성을 분석하고, 충분한 이웃 value가 있는 region만을 infer</ul><li>전체 image를 slice로 분할하여 각기 전송하고, 특정 단계가 실행 가능하다면 실행하면서 offloading latency를 낮춤</ul></ul><h3 id="1-dependency-aware-inference">1. Dependency Aware Inference</h3><ul><li><p>단순히 slice 기반으로 inference한 이후 merge를 수행하면 경계지점에서 잘못된 결과를 초래할 수 있음</p><li><p><em>Dependency Aware Inference</em> 기술 도입 : input feature point의 양이 충분한 region의 feature point만을 계산하는 기법</p><ul><li>종속성 : convolutional layer에 의해 발생 // 각 frame slice의 경계를 둘러싼 feature 계산에도 인접한 slice가 필요<li>경계 feature 연산 : last convolution layer에서 추가적인 pixel이 필요함</ul><li><p>병렬로 inference하는 방법 : 다음 slice를 받았을 때 특정 region을 재연산</p><ul><li>추가 연산 소요 // inference latency 팽창</ul><li><p>종속성 문제 해결법 : 각 layer의 output feature map에 대한 <em>valid region</em>의 크기를 먼저 계산하고, 그것에 기반하여 infer</p><ul><li><p>valid region : 가능한 input feature이 출분한 각 convolutional feature map의 영역으로 정의</p><ul><li><p>크기<br /> \(H_{i}^{out}=(H_{i}^{in}-1)/S+1\)</p>\[W_{i}^{out}= \begin{cases} \frac{W_{i}^{in}-(F-1)/2-1}{S}+1, &amp;\text{i=1,2,...,n-1}\\ \frac{W_{i}^{in}-1}{S}+1, &amp;{i=n} \end{cases}\]<ul><li>$H_{i}^{out}$, $W_{i}^{out}$ : edge cloud에 $i$ slice가 도착했을 때, convolution layer에서의 outpur feather map의 <u>valid region의 높이 / 넓이</u><li>$F$, $S$ : convolution 공간 범위 / 보폭</ul></ul></ul><li><p>컨셉</p><ul><li>1개의 frame을 4개로 분할함 ( $n=4$ )<ul><li>width는 전체 frame의 1/4, height는 고정<li>$H_{i}^{out}$ : 상수 취급 // $H_{i}^{in}$, $S$의 값에만 의해 영향을 받음<li>$W_{i}^{out}$ : slice가 도착할 때마다 값이 커짐</ul><li>slice의 가장 오른쪽 열은 계산하지 않음 ( 이후 slice에 의해 input feature의 종속성이 생길 수 있음 )<ul><li>더 많은 slice가 들어올수록 vaild region이 계속해서 증가하기 때문<li>slice가 들어올수록 <strong>기존에 들어온 slice에서의 input feature가 감소</strong> -&gt; 더 적은 연산만을 필요로함</ul><li>모든 slice가 들어온 이후에는 남아있는 모든 input feature를 연산</ul></ul><h2 id="motion-vectors-based-object-tracking">Motion Vectors based Object Tracking</h2><ul><li>인코딩된 video frame에서의 motion vector + 이전에 offload된 frame에서 캐시된 객체탐지 결과 =&gt; <strong>현재 frame에서의 객체탐지 결과 추정치</strong><li>모션 벡터 : video의 높은 압축률을 달성하기 위해 <strong>frame 간의 pixel offset을 표현하는 방법</strong><li>과정<ul><li>새로운 frame이 캡처되면 <em>Dynamic RoI Encoding</em> 단계로 전송<ul><li>encoder : frame 간의 압축을 위해 마지막에 cache된 탐지 결과에 대응하는 frame 사용</ul><li>system : 인코딩된 frame에서 모든 motion vector 추출<li>현재 frame에서의 객체를 추적하기 위해, bounding box를 motion vector의 평균치만큼 이동시킴</ul><li>human keypoint 추적에도 이 방식을 비슷하게 적용할 수 있었음<li>레퍼런스 frame과 현재 frame의 시간 간격이 길어질수록 정확도가 떨어짐<ul><li>latency를 매우 낮추었기 때문에, 정확도 있는 객체탐지 결과를 보여줄 수 있었음</ul><li>latency를 줄였다 -&gt; <strong>AR기기가 가상 오버레이를 높은 품질로 렌더링할 수 있다</strong> ( 충분한 시간과 자원을 남길 수 있기 때문 )</ul><h2 id="adaptive-offloading">Adaptive Offloading</h2><ul><li>효율적으로 offloading pipeline을 scheduling하기 위함<ul><li>인코딩된 frame을 edge로 보내야하는가?</ul><li>2가지 원리에 의해 작동함<ul><li>이전 frame이 edge cloud에 의해 완전히 수신되었을 경우에만 frame을 offload할 수 있음<ul><li>네트워크 혼잡을 피하기 위해 frame queueing을 없앰<li>실현하기 위해서는 이전 frame의 transmission latency를 계산해야함<ul><li>edge -&gt; AR device로 slice를 받았다는 신호를 발송<li>reception 시간과 transmission 시간의 차이를 이용하여 latency 계산<li>이것을 이용하여 다음 frame을 offload할 것인지 결정</ul></ul><li>현재 frame이 이전 frame과 상당히 다른 점이 있을 때에만 frame을 offload함<ul><li>소통 / 연산 cost를 최소화하기 위해 변화가 상당히 있는 필요한 view만을 offload함<li>실현하기 위해서는 두 frame 간의 차이를 계산해야함<ul><li>2가지 원칙을 충족시키는 2가지 관점으로 평가 ( 둘 중 하나만 만족해도 된다? )<ul><li>frame 사이에서 큰 motion이 발생했는지 ( user / object의 motion 모두 포함 )<li>frame에 나타나는 상당한 양의 변화된 pixel이 있는지</ul><li>frame의 motion : 모든 motion vector의 합으로 정량<li>new pixel의 수 : 인코딩된 frame의 intra-predicted macroblock 수로 정량</ul><li>인코딩된 frame에서의 2개의 type의 macroblock 사이에서, <strong>intra-predicted block은 새로 나타난 region을 레퍼런스로 잡는 경향이 있음</strong><ul><li>macroblock이 인코딩되는 동안 레퍼런스 frame에서 레퍼런스 pixel block을 찾지 못함</ul></ul><li>2가지 원리를 만족해야만 edge cloud로 frame을 offload</ul></ul><h2 id="implementation">Implementation</h2><ul><li>상용 하드웨어에서 동작할 수 있도록 구현하였음</ul><h3 id="1-hardware-setup">1. Hardware Setup</h3><ul><li>AR 기기 : mobile development board인 Nvidia Jetson TX2 사용<ul><li>Magic Leap One AR glass와 같은 mobile SoC를 사용<li>WiFi 연결 : TP-Link AC1900 라우터 사용</ul><li>edge cloud : PC 사용<ul><li>Intel i7-6850K CPU<li>Nvidia Titan XP GPU<li>라우터와 1Gbps의 이더넷으로 연결</ul><li>2개의 device 모두 Ubuntu 16.04 OS 사용</ul><h3 id="2-software-implementation">2. Software Implementation</h3><ul><li>고안한 기술들을 아래의 list에 기반하여 제작<ul><li>Nvidia JetPack<li>Nvidia Multimedia API<li>Nvidia TensorRT<li>Nvidia Video Codec SDK</ul></ul><h5 id="client-side">client side</h5><ul><li>client측 기능을 Nvidia Jetson TX2에 구현 ( JetPack SDK 사용 )<ul><li>기본 설계도를 따름</ul><li>카메라 캡처 session 제작 : JetPack 카메라 API를 이용하여 60fps에서 동작<li>비디오 인코더를 frame consumer로 등록 : 멀티미디어 API 이용<ul><li>RoI 인코딩 모듈을 알아야 함 -&gt; edge cloud에서 생성된 RoI를 기반으로 다음 frame을 인코딩해야함<ul><li><code class="language-plaintext highlighter-rouge">setROIParams()</code> 함수 사용 : RoI와 QP delta value 설정</ul></ul><li>외부 RPS control mode 이용 : 각 frame의 reference frame을 source frame의 현재 캐시된 탐지 결과로 설정<ul><li>추출된 motion vector로 캐시된 탐지 결과를 shift할 수 있음</ul><li><em>Parallel Streaming and Inference</em> 모듈 구현해야함 : 비디오 인코더의 slice mode를 활성화<ul><li><code class="language-plaintext highlighter-rouge">setSliceLength()</code> 함수 사용 : 적절한 길이를 전해주어 인코더가 frame을 4개의 slice로 분할할 수 있도록 함<li><code class="language-plaintext highlighter-rouge">getMetadata()</code> 함수 사용 : slice 인코딩 후, 각 slice에서 motion vector와 macroblock type을 얻음<ul><li>2개의 다른 thread ( 렌더링 / offloading ) 에서 각각 <em>Adaptive Offloading</em> / <em>MvOT</em> 의 input으로 사용<ul><li>offloading thread : 현재 frame을 offload할 것이라고 정했다면, 4개의 slice로 분할하여 무선 link를 통해 하나하나 server로 전달할 것<li>렌더링 thread : 모션벡터 기반 빠른 object tracking 기법이 추출된 모션벡터와 이전 결과 캐시값을 이용하여 fast object tracking 실현<ul><li>이후 탐지 결과에 따라 그 위치에 가상 오버레이를 렌더링</ul></ul></ul></ul></ul><h5 id="server-side">server side</h5><ul><li>2개의 main module 포함 ( 서로 다른 thread에서 동작하도록 설계되어 <strong>서로 block하지 않음</strong> )<ul><li>Parallel Decoding<ul><li>AR 기기로부터 slice 입력을 받을 때까지 계속 wait<li>slice를 받으면 video decoder에게 <u>비동기식 mode로 디코드 지시</u> -&gt; system을 block하지 않음<li>Nvidia Video Codec SDK 사용 ( Titan Xp GPU의 비디오 디코더의 하드웨어 가속 이점을 챙기기 위함 )<li>각 slice를 encode하면 인코더에 부착된 <code class="language-plaintext highlighter-rouge">callback()</code> 함수에 의해 inference thread로 전달됨</ul><li>Parallel Inference<ul><li>Nvidia TensorRT 사용 ( Nvidia GPU에서 딥러닝 추론 optimizer로써 높은 성능을 보임 )<li>서버측 PC의 inference latency의 한계를 넘기 위해 INT8 calibration tool 사용 -&gt; 객체탐지 모델 optimize / 같은 setup에서 3~4배의 latency 효율<li><em>Dependency Aware Inference</em> 메소드 달성해야함 : <code class="language-plaintext highlighter-rouge">PluginLayer</code>를 convolution / pooling layer에 추가<ul><li>방정식 (2)에 기반하여 region의 input / output 크기를 조정하기 위함</ul><li>탐지한 결과 ( QP map ) 을 AR 기기로 전송 ( 다음에 쓰일수도 있음 )</ul></ul></ul><h2 id="evaluation">Evaluation</h2><ul><li>시스템의 전체 성능 평가<ul><li>detection 정확도<li>detection latency<li>end-to-end tracking and rendering latency<li>offloading latency<li>bandwidth 소모<li>resource 소모</ul><li>이 시스템은 높은 정확도와 낮은 latency를 이루었음<li>결과<ul><li>탐지 정확도 향상 ( 20.2% ~ 34.8% )<li>false detection rate 낮춤 ( 27% ~ 38.2% )<li>offload latency 낮춤 ( 32.4% ~ 50.1% )<li>MvOT 메소드에서 평균 2.24ms 시간 소요 ( 렌더링할 시간과 리소스 남김 )</ul></ul><h3 id="1-experiment-setup">1. Experiment Setup</h3><ul><li>이전에 언급한 실험 환경을 동일하게 사용<li>2개의 다른 detection task 설계 ( 성능실험 )<ul><li>object detection task<ul><li>edge에서 Faster R-CNN object detection model 수행<li>각 offload frame에서 객체의 bounding box를 생성</ul><li>keypoint detection task<ul><li>edge에서 Keypoint Detection Mask R-CNN model 수행<li>human body keypoint 탐지</ul></ul><li>결과를 탐지하여 AR 기기가 user의 왼손에 cube를 렌더링할 것<li>AR 기기는 60fps 설정으로 모델을 돌림<li>AR 기기와 server 연결 WiFi 설정은 2가지 ( 2.4GHz, 5GHz )<ul><li>bandwidth : 82.8Mbps / 276Mbps</ul></ul><h3 id="2-object-detection-accuracy">2. Object Detection Accuracy</h3><ul><li><p>시스템 : 변화하는 네트워크 환경에서 <u>높은 정확도 + 낮은 false detection 확률</u>을 달성</p><li><p>4가지 접근법으로 탐지 정확도 측정</p><ul><li>기본 솔루션 ( baseline )<li>기본 솔루션 + 2가지의 latency optimization 기술<li>기본 솔루션 + 모션벡터 기반 메소드<li>기본 솔루션 + 모든 고안된 기술</ul><li><p>2개의 주요 metric으로 탐지 정확도 평가</p><ul><li>탐지 정확도 평균치<li>false detection rate</ul><li><p>60fps로 각 video의 추출된 frame을 client측 video encoder에게 공급하여 카메라를 emulate</p><ul><li>그러나 video frame에서의 반복 가능한 motion experiment를 허용</ul><li><p>각 frame에서의 탐지 정확도 계산 방법 : MvOT와 ground truth detection result 사이의 IoU나 OKS 평균치를 계산</p><ul><li>OKS : object keypoint similarity<ul><li>keypoint의 탐지된 위치, ground truth label 사이의 정규화 유클리드 distance로 묘사 ( 0 ~ 1 )</ul><li>IoU : detection bounding box와 ground truth bounding box의 교차면적</ul><li><p>정확도 향상 결과</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">task<th style="text-align: center">WiFi 2.4GHz<th style="text-align: center">WiFi 5GHz<tbody><tr><td style="text-align: center">object detection case<td style="text-align: center">23.4% 향상<td style="text-align: center">20.2% 향상<tr><td style="text-align: center">human keypoint detection case<td style="text-align: center">34.8% 향상<td style="text-align: center">24.6% 향상</table></div><ul><li>low latency offloading 기술 / fast object tracking 기술 모두 latency를 줄임으로써 <strong>정확도 향상에 기여</strong></ul><li><p>측정한 detection accuracy를 CDF로 그림</p><ul><li>수용 가능한 정확도를 정하기 위해 2개의 threshold 값을 도입 ( 컴퓨터 비전에서 사용됨 )<ul><li>0.5 ( 느슨함 )<li>0.75 ( 엄격함 )</ul><li>탐지 정확도가 정확도 metric보다 낮은 detected bounding box / set of keypoint : false detection으로 간주<li>AR / MR 시스템에서의 높은 품질 요구 : 엄격한 정확도 metric을 사용할 것 ( 0.75 threshold )</ul><li><p>결과</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">task<th style="text-align: center">WiFi 2.4GHz<th style="text-align: center">WiFi 5GHz<tbody><tr><td style="text-align: center">object detection ( false rate )<td style="text-align: center">10.68%<td style="text-align: center">4.34%<tr><td style="text-align: center">object detection ( reduce rate )<td style="text-align: center">33.1%<td style="text-align: center">27%<tr><td style="text-align: center">human keypoint detection ( false rate )<td style="text-align: center"> <td style="text-align: center"> <tr><td style="text-align: center">human keypoint detection ( reduce rate )<td style="text-align: center">38.2%<td style="text-align: center">34.9%</table></div><ul><li>추가로, delay에 따라서 결과가 제대로 오버레이되지 않을 수 있음</ul></ul><h5 id="impact-of-background-network-traffic">impact of background network traffic</h5><ul><li><p>고안된 시스템은 background의 네트워크 부하에 영향을 크게 받지 않는다</p><li><p>네트워크 부하 실험 ( 0% -&gt; 90%로의 load 추가 )</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">system<th style="text-align: center">WiFi 2.4GHz<th style="text-align: center">WiFi 5GHz<tbody><tr><td style="text-align: center">baseline<td style="text-align: center">49.84%<td style="text-align: center">35.6%<tr><td style="text-align: center">고안된 system<td style="text-align: center">21.97%<td style="text-align: center">15.58%</table></div></ul><h3 id="3-obejct-tracking-latency">3. Obejct Tracking Latency</h3><ul><li><p>이전 탐지 object를 새로운 frame에 위치를 조정하는 데 걸리는 시간 : 2.24ms</p><ul><li>AR 기기가 충분한 시간과 resource를 확보 가능<li>frame 간격 사이에 높은 품질의 가상 오버레이를 렌더링할 수 있음</ul><li><p>다른 기술과의 비교</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">technique<th style="text-align: center">tracking latency<tbody><tr><td style="text-align: center">MvOT<td style="text-align: center">2.24ms<tr><td style="text-align: center">OF-LK<td style="text-align: center">8.53ms<tr><td style="text-align: center">OF-HS<td style="text-align: center">79.01ms</table></div><ul><li>MvOT : 약 75%의 GPU resource를 아낌<li>다른 기술들이 MvOT보다 정확도가 높지만, <strong>latency 때문에 정확도가 떨어짐</strong></ul></ul><h3 id="4-end-to-end-tracking-and-rendering-latency">4. End-to-End Tracking and Rendering Latency</h3><ul><li>smooth한 AR experience를 하기 위해서는 60fps에서는 16.7ms의 inter-frame time안에 end-to-end latency가 있어야 한다<ul><li><strong>1s를 60fps로 나눈 16.7ms마다 결과를 보여줄 수 있어야 한다</strong></ul><li>기본적으로 MvOT를 이용한 latency가 2.24ms이므로, 렌더링 등에 쓰일 수 있는 latency가 14ms나 됨<li>16.7ms 안에 모든 절차를 수행할 수 있고 높은 품질의 AR experience를 제공할 수 있음</ul><h3 id="5-offloading-latency">5. Offloading Latency</h3><ul><li>RoI 인코딩 기술 + Parallel Streaming and Inference 기술 : offload latency를 획기적으로 줄일 수 있음<li>offloading latency = streaming latency + inference latency ( 2개는 parallel하게 동작함 )<li>평균 인코딩 latency : 1.6ms // 이 system을 사용하면 1ms 미만으로 줄일 수 있음<li>결과<ul><li>baseline에서는 offloading latency : 34.56ms ( 2.4GHz ) / 22,96ms ( 5GHz )<li>RDE 추가 : streaming latency를 8.33ms / 2.94ms로 낮출 수 있음<li>RDE + PSI 추가 : 전체 offloading latency를 17.23ms / 15.52ms로 낮출 수 있음</ul><li>이 기술은 <strong>bandwidth가 작은 연결에서 offloading latency를 획기적으로 줄일 수 있음</strong></ul><h3 id="6-bandwidth-consumption">6. Bandwidth Consumption</h3><ul><li><em>DRE</em> 기술 + <em>Adaptive Offloading</em> 기술 : bandwidth 소모를 줄일 수 있음<li>bandwidth 소모 측정 ( 3가지 관점 )<ul><li>baseline<li>DRE 추가<li>DRE + adaptive offloading</ul><li>각 실험에서 frame을 인코딩하기 위한 base 품질을 제어하기 위한 QP 제공 ( 5, 10, 15, 20, 25, 30, 35 )<li>DRE : 검출된 RoI를 기준으로 인코딩 품질을 조정할 것<li>adaptive offloading : 각 frame을 edge cloud로 보낼 것인지 결정<li>결과<ul><li>DRE + adaptive offloading 기술을 사용한 것이 같은 bandwidth 사용량 당 정확도가 가장 높았음<li>유사하게, 정확도를 같이 맞춘다면 ( 0.9 ), bandwidth 소모량을 baseline보다 62.9% 낮출 수 있었음</ul></ul><h3 id="7-resource-comsumption">7. Resource Comsumption</h3><ul><li>고안된 시스템은 AR 기기에서의 계산 resource를 매우 적게 사용함<li>객체탐지 task 실행 ( 어떠한 렌더링 task 없이 순수하게 )<ul><li>20분 동안의 DrivingPOV 영상 사용<li>JetPack의 <em>tegrastats</em> tool 사용 : CPU / GPU의 사용량 측정 목적</ul><li>결과<ul><li>고안된 시스템이 기존에 비해 15%의 CPU자원 / 13%의 GPU자원만을 사용함<li>나머지 resource는 남겨져 AR / MR 시스템에서 높은 품질의 그래픽을 렌더링할 때 사용됨</ul></ul><h2 id="related-work">Related Work</h2><h5 id="mobile-ar">mobile AR</h5><ul><li>모바일 AR 시스템 설계 : 산업 / 아카데미아 분야의 강한 interest<li>ARCore, ARKit : 모바일 AR 플랫폼<li>HoLoLens, Magic Leap One : MR를 실현할 수 있을 것으로 기대되는 플랫폼<li><strong>하지만 어떤 플랫폼도 object detection을 지원하지 않음</strong> ( 연산 요구가 높음 )<ul><li>문제 해결을 위한 플랫폼이 나오기 시작<li>Vuforia : 기존 feature extraction 접근법을 기반으로 모바일 기기에 object detection plugin 제공<li>Overlay : 모바일 기기의 센서 data 사용 -&gt; 후보 object의 개수를 줄임<li>VisualPrint : 추출된 feature만을 cloud로 보내 image offloading의 bandwidth 소모를 줄임<li><strong>하지만 위의 기술들은 real-time ( 30fps / 60fps ) 에서 작동하지 못함</strong><ul><li>Glimpse : 연속적 object 인식 시스템<ul><li>도전적인 network 조건에서 cloud offload를 실행하도록 설계됨<li>30fps에서 실행됨<li>trigger frame만을 cloud로 offload<li>시각적 flow 기반 object tracking 방법 사용 -&gt; 나머지 frame의 object bounding box를 업데이트<li>시간과 resource를 많이 씀 ( 렌더링할 조건 부족 )</ul><li>위와는 다른 부분 탐구 : 설계 공간에서 근처의 edge server에 대한 보다 나은 network latency 안에서 더 높은 품질의 AR / MR을 실현할 수 있는 다른 지점 탐색</ul></ul><li>고안된 기술들을 활용해 offload latency를 줄임 -&gt; 하나의 frame interval에 끝낼 수도 있음<ul><li>low-cost의 MvOT : device에 렌더링할 수 있도록 시간과 resource 남김</ul></ul><h5 id="deep-learning">deep learning</h5><ul><li>CNN이 최근 기존 hand-craft feature 접근법보다 더 나은 성능 제공<li>Huang et al : 기존 CNN 모델 ( Faster R-CNN, R-FCN, SSD 등 ) 들의 speed / 정확도 tradeoff를 비교<li>멀티태스킹 학습 : 보다 미세한 검출을 위해 object bounding box 내부의 깊은 특징을 더 재사용할 수 있음<li>모바일 장치에서 CNN 모델을 효율적으로 실행하는 방법에 대한 연구는 좀 있음<ul><li><strong>그 어떤 것도 고품질 AR / MR 시스템에 대한 latency를 충족할 수 없었음</strong></ul></ul><h5 id="mobile-vision-offloading">mobile vision offloading</h5><ul><li></ul><h5 id="adaptive-video-streaming">adaptive video streaming</h5><ul><li></ul><h2 id="discussion">Discussion</h2><h5 id="generality">generality</h5><ul><li></ul><h5 id="comparison-with-existing-ar-tools">comparison with existing AR tools</h5><ul><li></ul><h5 id="limitation">Limitation</h5><ul><li></ul><h2 id="conclusion">Conclusion</h2><ul><li>60fps에서 동작하는 AR / MR object detection을 높은 정확도를 이끌어낼 수 있는 시스템을 만들었음<ul><li>실현하기 위해 여러 low latency offloading 기술을 고안<li>AR 기기에서는 렌더링 pipeline을 offloading pipeline과 분리함<li>fast object tracking 메소드를 사용하여 탐지 정확도를 유지하려함</ul><li>상용 하드웨어에 시스템 prototype 구현<ul><li>정확도 향상 ( 20.2% ~ 34.8% )<li>false detection 확률 감소 ( 27% ~ 38.2% )</ul><li>AR 기기에서 objec tracking을 하는 것이 resource를 매우 적게 요구함<ul><li>frame 사이의 렌더링 시간을 꽤 많이 남길 수 있고, 높은 품질의 AR / MR experience를 가능하게함</ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/imes/'>IMES</a>, <a href='/categories/paper/'>paper</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/imes/" class="post-tag no-text-decoration" >IMES</a> <a href="/tags/paper/" class="post-tag no-text-decoration" >paper</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Edge Assisted Real-time Object Detection for Mobile Augmented Reality - Joe2357&url=https://joe2357.github.io/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Edge Assisted Real-time Object Detection for Mobile Augmented Reality - Joe2357&u=https://joe2357.github.io/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Edge Assisted Real-time Object Detection for Mobile Augmented Reality - Joe2357&url=https://joe2357.github.io/posts/Edge-Assisted-Real-time-Object-Detection-for-MAR/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/SCPC-2021-Round-1/">SCPC 2021 1라운드 후기</a><li><a href="/posts/SCPC-2020-Round-1/">SCPC 2020 1라운드 후기</a><li><a href="/posts/728/">Codeforces Round #728 (Div. 2) 후기</a><li><a href="/posts/727/">Codeforces Round #727 (Div. 2) 후기</a><li><a href="/posts/726/">Codeforces Round #726 (Div. 2) 후기</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ps/">PS</a> <a class="post-tag" href="/tags/review/">Review</a> <a class="post-tag" href="/tags/codeforces/">Codeforces</a> <a class="post-tag" href="/tags/imes/">IMES</a> <a class="post-tag" href="/tags/paper/">paper</a> <a class="post-tag" href="/tags/scpc/">SCPC</a> <a class="post-tag" href="/tags/ds/">DS</a> <a class="post-tag" href="/tags/storage/">Storage</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/WebRTC/"><div class="card-body"> <span class="timeago small" > May 26 <i class="unloaded">2021-05-26T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Audio and Video Mixing Method to Enhance WebRTC</h3><div class="text-muted small"><p> 2021 / 7 / 21 IMES 세미나 Abstract WebRTC : 웹브라우저에 JavaScript API를 호출함으로써 P2P 라이브 스트리밍 제공 소수의 peer로 제한되는 프로토콜 ( 다중 peer들의 real-time 스트림을 mix하기 힘듬 / mix된 스트림을 많은 수의 audience에게 분배할 수...</p></div></div></a></div><div class="card"> <a href="/posts/PBE-CC/"><div class="card-body"> <span class="timeago small" > Nov 26, 2020 <i class="unloaded">2020-11-26T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurement</h3><div class="text-muted small"><p> 2021 / 1 / 20 IMES 세미나 Abstract cellular network는 복잡해지고 과밀되어짐 delay, jitter 등의 문제 발생 PBE-CC : sender가 정확하고 급격하게 반응할 수 있도록 하는 최신 5G radio 혁신 기반 congestion control 알고리즘 ...</p></div></div></a></div><div class="card"> <a href="/posts/MPBond/"><div class="card-body"> <span class="timeago small" > Jan 21 <i class="unloaded">2021-01-21T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MPBond: Efficient Network-level Collaboration Among Personal Mobile Devices</h3><div class="text-muted small"><p> 2021 / 2 / 17 IMES 세미나 Abstract MPBond : 여러 개인 mobile device가 공동으로 internet에서 content를 가져올 수 있도록 하는 효율적인 system 스마트워치 : data downloading을 통해 페어링된 스마트폰 지원 MPTCP ( Multipath...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/698/" class="btn btn-outline-primary" prompt="Older"><p>Codeforces Round #698 (Div. 2) 후기</p></a> <a href="/posts/EdgeProg/" class="btn btn-outline-primary" prompt="Newer"><p>EdgeProg: Edge-centric Programming for IoT Applications</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/username">Joe2357</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/ps/">PS</a> <a class="post-tag" href="/tags/review/">Review</a> <a class="post-tag" href="/tags/codeforces/">Codeforces</a> <a class="post-tag" href="/tags/imes/">IMES</a> <a class="post-tag" href="/tags/paper/">paper</a> <a class="post-tag" href="/tags/scpc/">SCPC</a> <a class="post-tag" href="/tags/ds/">DS</a> <a class="post-tag" href="/tags/storage/">Storage</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://joe2357.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
